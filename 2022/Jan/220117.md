# Day 98
## 파이썬으로 챗봇 만들기
### Chapter 2
#### 챗봇을 만들기 위해 자연어 처리를 알아야 하는 이유
자연어 처리(NLP, Natural Language Processing)는 컴퓨터가 인간의 언어를 분석하고 이해할 수 있도록 하는 인공지능의 한 분야다.

NLP를 알지 못하면 구현하려는 챗봇의 범위가 상당히 제약될 수 있다. 

컴퓨터가 사용자의 자연어를 이해하기 위해서는, 그것이 어떤 언어인지 혹은 어떤 형태(텍스트, 음성, 이미지 등)인지와 관계없이 NLP 알고리즘과 기술을 반드시 사용해야 한다.

#### spaCy란?
spaCy는 고급 NLP를 위한 오픈 소스 소프트웨어 라이브러리이며, 딥러닝 모델에 의해 훈련된 다양한 메소드에 접근할 수 있는 직관적인 API를 제공한다.

spaCy는 대규모 정보를 추출하는 데 매우 능숙하다. 

TensorFlow, PyTorch, scikit-learn, Gensim, 기타 파이썬 관련 여러 딥러닝 프레임워크들과 매끄럽게 연동된다.

**spaCy의 특징들**
- Non-destructive 토큰화
- 개체명 인식(Named entity recognition)
- 28개 이상의 다국어 지원
- 8개 언어에 대한 13가지 통계 모델
- 사전에 학습된 워드 벡터(word vector)
- 손쉬운 딥러닝 연동
- 형태소 분석
- 라벨화된 의존 구문 분석(Labeled dependency parsing)
- 구문 중심의 문장 분할(Syntax-driven sentence segmentation)
- 구문과 개체명 인식을 위한 탑재된 비주얼라이저(visualizers)
- 편리한 문자열-해시(string-to-hash) 매핑
- numpy data arrays 형식으로의 추출
- 효율적인 바이너리 직렬화(binary serialization)
- 간편한 모델 패키징과 배포
- 빠른 속도
- 견고하고 엄격하게 평가한 정확도

#### 챗봇 구축에 필요한 자연어 처리의 기본적인 방법
사용자로부터 입력받은 구문을 청크라는 단위로 분해하고, 각각이 의미를 가질 수 있도록 도와준다.

**형태소 분석(POS Tagging)**  
형태소 분석(Part-of-speech Tagging)이란, 어떤 문장을 읽고 그 문장을 구성하는 각각의 단어(혹은 토큰)에 명사, 동사, 형용사와 같은 품사를 할당하는 프로세스를 의미한다.

형태소 분석은 주어진 문장의 엔티티를 파악하고자 할 때 매우 중요하다. 챗봇 동작 시작 단계에서 가장 먼저 해야 할 일은 형태소 분석을 통해 주어진 문장에 무엇이 포함되어 있는지를 확인하는 것이다.
```python
# 예제 1
nlp = spacy.load('en_core_web_sm')               # spacy 영어 모델을 파이썬 오브젝트에 로드
doc = nlp(u'I am learning how to build chabots') # 토큰을 위한 doc 오브젝트 생성
for token in doc:
    print(token.text, token.pos)                 # 토큰과 형태소 분석 결과 출력
```
```markdown
**수행결과**
I 95
am 87
learning 100
how 98
to 94
build 100
chabots 92
```
```python
# 예제 2
doc = nlp(u'I am going to London next week for a meeting.')
for token in doc:
    print(token.text, token.pos_)
```
```markdown
**수행결과**
I PRON
am AUX
going VERB
to ADP
London PROPN
next ADJ
week NOUN
for ADP
a DET
meeting NOUN
. PUNCT
```
nlp메소드를 통해 doc 오브젝트에서 리턴된 토큰 및 각 토큰의 형태소 분석 결과를 예제 1, 2에서 확인할 수 있다.

이러한 분석 결과는 각 단어가 문법적으로 올바르게 사용될 수 있도록 단어 하나하나에 태그 형태로 소속되게 된다.

아래는 doc 오브젝트로부터 분리된 토큰의 또 다른 속성들을 탐구하기 위한 예제이다.
```python
# 예제 3
doc = nlp(u'Google release "Move Mirror" AI experiment that matches your pose from 80,000 images')

for token in doc:
    print(token.text, token.lemma_, token.pos_, token.tag_, token.dep_, token.shape_, token.is_alpha, token.is_stop, sep='\t')
```
**수행결과**  
Text|Lemma|POS|Tag|Dep|Shape|Alpha|Stop
:----------:|:--------:|:---------:|:-----:|:---------:|:-------:|:------:|:------:
Google|Google|PROPN|NNP|compound|Xxxxx|True|False
release|release|NOUN|NN|ROOT|xxxx|True|False
"|"|PUNCT|``|punct|"|False|False
Move|Move|PROPN|NNP|nmod|Xxxx|True|True
Mirror|Mirror|PROPN|NNP|appos|Xxxxx|True|False
"|"|PUNCT|''|punct|"|False|False
AI|AI|PROPN|NNP|compound|XX|True|False
experiment|experiment|NOUN|NN|appos|xxxx|True|False
that|that|PRON|WDT|nsubj|xxxx|True|True
matches|match|VERB|VBZ|relcl|xxxx|True|False
your|your|PRON|PRP$|poss|xxxx|True|True
pose|pose|NOUN|NN|dobj|xxxx|True|False
from|from|ADP|IN|prep|xxxx|True|True
80,000|80,000|NUM|CD|nummod|dd,ddd|False|False
images|image|NOUN|NNS|pobj|xxxx|True|False
```python
# 예제 4
doc = nlp(u'I am learning how to build chatbots')
for token in doc:
    print(token.text, token.lemma_, token.pos_, token.tag_, token.dep_, token.shape_, token.is_alpha, token.is_stop, sep='\t')
```
**수행결과**  
Text|Lemma|POS|Tag|Dep|Shape|Alpha|Stop
:----------:|:--------:|:---------:|:-----:|:---------:|:-------:|:------:|:------:
I|I|PRON|PRP|nsubj|X|True|True
am|be|AUX|VBP|aux|xx|True|True
learning|learn|VERB|VBG|ROOT|xxxx|True|False
how|how|SCONJ|WRB|advmod|xxx|True|True
to|to|PART|TO|aux|xx|True|True
build|build|VERB|VB|xcomp|xxxx|True|False
chatbots|chatbot|NOUN|NNS|dobj|xxxx|True|False


**출력된 각 속성들이 의미하는 것**  
TEXT|실제 텍스트 확은 단어
:------:|:-------------------------
LEMMA|처리되는 단어 형태
POS|단어의 품사
TAG|품사 + 추가정보(ex: 과거 시제, 현재 시제)
DEP|문맥 간 의존도(토큰들 간의 관계)
SHAPE|단어의 형태(대소문자, 부호, 숫자 등)
ALPHA|알파벳 여부
Stop|불용어(문장 내에서 크게 의미가 없는 단어) 여부

**POS 컬럼의 각 속성들이 의미하는 것**  
POS|DESCRIPTION|EXAMPLES
:------:|:-----------------------------:|:-------------------------------------
ADJ|adjective|big, old, green, incomprehensible, first
ADP|adposition|in, to, during
ADB|adverb|very, tomorrow, down, where, there
AUX|auxiliary|Is, has (done), will (do), should(do)
CONJ|conjunction|and, or, but
CCONJ|coordinating conjunction|and, or, but
DET|determiner|a, an, the
INTJ|interjection|psst, ouch, bravo, hello
NOUN|noun|girl, cat, tree, air, beauty
NUM|numeral|1, 2022, one, seventy-seven, IV, MMXIV
PART|particle|'s, not,
PRON|pronoun|I, you, he, she, myself, themselves, somebody
PUNCT|punctuation|., (, ), ?
SCONJ|subordinating conjunction|if, while, that
SYM|symbol|$, %, +, -
VERB|verb|run, runs, running, eat, ate, eating
X|other|sfpksdpsxmsa
SPACE|space| 

챗봇에 형태소 분석이 필요한 이유는 사용자가 입력한, 챗봇이 아직 학습하지 않았거나 덜 학습된, 텍스트의 복잡성을 감소시켜 이해가 용이하도록 만든다.

**어간 추출(Stemming)과 표제어 추출(Lemmatization)**  
어간 추출이란 주어진 단어에서 그 단어의 기본적인 형태인 어간을 추출하는 과정을 의미한다.  
예를 들면 "saying"이란 단어는 "say"로 변경되고, "presumably"는 "presum"이 란 단어로 변하는 것인데, 추출 결과가 항상 정확하지 않을 수도 있다.

표제어 추출은 어간 추출과 유사해 보이지만, 표제어 추출은 어떤 단어가 의미하고 있는 표제어(기본 사전형 단어)를 찾아가는 과정이라는 점에서 서로 다르다고 할 수 있다.  
예를 들어 "to walk"라고하는 단어는 "walk", "walked", "walks" 혹은 "walking"과 같이 다양하게 표현될 수 있는데 이때 사전에서 찾아볼 수 있는 "walk"를 그 단어들의 표제어(lemma)라고 부른다.

spaCy는 표제어 추출이 더 정확하고 생산적이라고 판단하여 어간 추출기는 내장하고 있지 않다.

어간 추출과 표제어 추출의 차이점은 다음과 같다.
- **어간 추출**은 먼저 추출 작업을 정교하지 않게, 휴리스틱(사람들이 빠르게 사용할 수 있는 어림짐작의 방법)한 방법으로 진행한다. 단어를 잘라내고 남은 단어가 실제로 의미 있는 단어라고 생각하고 작업이 진행되지만, 단어의 의미를 내포한 핵심 부분(derivational affixes)이 제거되는 경우도 종종 발생한다.

- **표제어 추출**은 어휘 분석 및 형태 분석을 이용하여 조금 더 명확하게 추출 작업을 진행한다. 이것은 의미 없는 부분만을 제거하고 표제어라고 하는 단어의 사전적 형태만을 리턴하기 위해 최선을 다한다.

```python
# 예제
nlp = spacy.load('en_core_web_sm')
test_word = nlp(u'chuckles')
print(test_word[0].text, test_word[0].lemma_)

test_word = nlp(u'blazing')
print(test_word[0].text, test_word[0].lemma_)

test_word = nlp(u'fastest')
print(test_word[0].text, test_word[0].lemma_)
```
```markdown
**수행결과**
chuckles chuckle
blazing blaze
fastest fast
```

NLTK는 자연어 처리 라이브러리 중 하나이다. NLTK에서 제공하는 두 가지 어간 추출 기술인 PorterStemmer와 SnowBallStemmer를 사용한 것을 보자.
```python
from nltk.stem.porter import *
from nltk.stem.snowball import SnowballStemmer
porter_stemmer = PorterStemmer()
snowball_stemmer = SnowballStemmer("english")
print(porter_stemmer.stem("fastest"))
print(snowball_stemmer.stem("fastest"))
```
```markdown
**수행결과**
fastest
fastest
```
"fastest"의 어간을 추출하는 코드이다. 두 가지 방법 모두 "fastest"를 출력하는 것을 확인할 수 있다.

단어의 사전적 형태가 필요할 경우에는 표제어 추출을 해야 한다.

구글의 검색 엔진도 표제어 추출이 이용된 사례이다.  
"왕자의 게임 다음 시즌이 언제 출시되지?"라는 문장을 검색한다고 생각해보자. 검색엔진이 이 문장을 그대로 이용하여 검색 결과를 찾는다고 가정하면 "왕좌의 게임 다음 시즌 개봉일"이라는 제목의 기사와는 연결시키지 못할 것이다. 하지만 사용자가 입력한 문장과 연관된 결과를 찾기 전에 표제어 추출을 진행했다면, 더 나은 결과를 표현할 수 있을 것이다.