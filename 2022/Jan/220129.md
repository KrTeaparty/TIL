# Day 109
# OpenCV(Computer Vision)
## 프로젝트 : 얼굴을 인식하여 캐릭터 씌우기
### 특정 위치 추출
```python
import cv2
import mediapipe as mp

# 얼굴을 찾고, 찾은 얼굴에 표시를 해주기 위한 변수 정의
mp_face_detection = mp.solutions.face_detection # 얼굴 검출을 위한 face_detection 모듈을 사용
mp_drawing = mp.solutions.drawing_utils # 얼굴의 특징을 그리기 위한 drawing_utils 모듈을 사용

# 동영상 파일 열기
cap = cv2.VideoCapture('face_video.mp4')

with mp_face_detection.FaceDetection(model_selection=0, min_detection_confidence=0.7) as face_detection:
    while cap.isOpened():
        success, image = cap.read()
        if not success:
            break

        # To improve performance, optionally mark the image as not writeable to
        # pass by reference.
        image.flags.writeable = False
        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
        results = face_detection.process(image)

        # Draw the face detection annotations on the image.
        image.flags.writeable = True
        image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)
        if results.detections:
            # 6개의 특징 : 오른쪽 눈, 왼쪽 눈, 코 끝부분, 입 중심, 오른쪽 귀, 왼쪽 귀 (귀구슬점, 이주)
            for detection in results.detections:
                #mp_drawing.draw_detection(image, detection)
                
                # 특정 위치 가져오기
                keypoints = detection.location_data.relative_keypoints
                right_eye = keypoints[0] # 오른쪽 눈
                left_eye = keypoints[1] # 왼쪽 눈
                nose_tip = keypoints[2] # 코 끝부분
                
                h, w, _ = image.shape # height, width, channel : 이미지로부터 세로, 가로 크기 가져옴
                right_eye = (int(right_eye.x * w), int(right_eye.y * h)) # 이미지 내에서 실제 좌표 (x, y)
                left_eye = (int(left_eye.x * w), int(left_eye.y * h))
                nose_tip = (int(nose_tip.x * w), int(nose_tip.y * h))
                
                # 양 눈에 동그라미 그리기
                cv2.circle(image, right_eye, 50, (255, 0, 0), 10, cv2.LINE_AA)
                cv2.circle(image, left_eye, 50, (0, 255, 0), 10, cv2.LINE_AA)
                cv2.circle(image, nose_tip, 75, (0, 255, 255), 10, cv2.LINE_AA)
                
        cv2.imshow('MediaPipe Face Detection', cv2.resize(image, None, fx=0.5, fy=0.5))
        if cv2.waitKey(1) == ord('q'):
            break
            
cap.release()
cv2.destroyAllWindows()
```
model_selection은 0 또는 1을 선택할 수 있는데 0은 카메라로부터 2미터 이내의 거리, 1은 5미터 이내의 거리의 face detection에 적합하다.

min_detection_confidence 어느 정도 퍼센트 이상이 되면 얼굴로 인식할지 정한다. 

relative_keypoints의 x, y좌표는 0.xxx 이런 식으로 되어 있다. 따라서 이미지 내의 실제 좌표로 변환해줘야 한다.

![circle_on_face](https://user-images.githubusercontent.com/38313522/151653541-cf73d617-bccc-4103-971c-61284565c040.PNG)

### 그림판 이미지 씌우기
```python
import cv2
import mediapipe as mp

# 얼굴을 찾고, 찾은 얼굴에 표시를 해주기 위한 변수 정의
mp_face_detection = mp.solutions.face_detection # 얼굴 검출을 위한 face_detection 모듈을 사용
mp_drawing = mp.solutions.drawing_utils # 얼굴의 특징을 그리기 위한 drawing_utils 모듈을 사용

# 동영상 파일 열기
cap = cv2.VideoCapture('face_video.mp4')

# 이미지 불러오기
image_right_eye = cv2.imread('right_eye.png') # 100 x 100
image_left_eye = cv2.imread('left_eye.png') # 100 x 100
image_nose = cv2.imread('nose.png') # 300 x 100 (가로, 세로)

with mp_face_detection.FaceDetection(model_selection=0, min_detection_confidence=0.7) as face_detection:
    while cap.isOpened():
        success, image = cap.read()
        if not success:
            break

        # To improve performance, optionally mark the image as not writeable to
        # pass by reference.
        image.flags.writeable = False
        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
        results = face_detection.process(image)

        # Draw the face detection annotations on the image.
        image.flags.writeable = True
        image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)
        if results.detections:
            # 6개의 특징 : 오른쪽 눈, 왼쪽 눈, 코 끝부분, 입 중심, 오른쪽 귀, 왼쪽 귀 (귀구슬점, 이주)
            for detection in results.detections:
                #mp_drawing.draw_detection(image, detection)
                
                # 특정 위치 가져오기
                keypoints = detection.location_data.relative_keypoints
                right_eye = keypoints[0] # 오른쪽 눈
                left_eye = keypoints[1] # 왼쪽 눈
                nose_tip = keypoints[2] # 코 끝부분
                
                h, w, _ = image.shape # height, width, channel : 이미지로부터 세로, 가로 크기 가져옴
                right_eye = (int(right_eye.x * w) - 20, int(right_eye.y * h) - 100) # 이미지 내에서 실제 좌표 (x, y)
                left_eye = (int(left_eye.x * w)+ 20, int(left_eye.y * h) - 100)
                nose_tip = (int(nose_tip.x * w), int(nose_tip.y * h))
                
                # 각 특징에다가 이미지 그리기
                image[right_eye[1] - 50:right_eye[1] + 50, right_eye[0] - 50:right_eye[0] + 50] = image_right_eye 
                image[left_eye[1] - 50:left_eye[1] + 50, left_eye[0] - 50:left_eye[0] + 50] = image_left_eye 
                image[nose_tip[1] - 50:nose_tip[1] + 50, nose_tip[0] - 150:nose_tip[0] + 150] = image_nose
                
        cv2.imshow('MediaPipe Face Detection', cv2.resize(image, None, fx=0.5, fy=0.5))
        if cv2.waitKey(1) == ord('q'):
            break
            
cap.release()
cv2.destroyAllWindows()
```
각 특징에다가 이미지 그리기 부분은 이전에 crop을 활용한 부분이다. image의 해당 부분을 다른 이미지로 바꾸는 것이다.

![painter_on_face](https://user-images.githubusercontent.com/38313522/151653988-f6c3e6df-8ba6-4304-a44e-b9263dc1932d.PNG)

### 캐릭터 이미지 씌우기
```python
import cv2
import mediapipe as mp

def overlay(image, x, y, w, h, overlay_image): # (대상 이미지, x, y, width, height, 덮어씌울 이미지(4채널))
    alpha = overlay_image[:, :, 3] # BGRA에서 A값만 가져온다.
    mask_image = alpha / 255 # 0 ~ 255의 범위를 가지는 것을 0 ~ 1 사이의 값을 가지도록 변경 (0 : 투명, 1 : 불투명)
    # (255, 255) -> (1, 1)
    # (255, 0)      (1, 0)
    
    # 1 - mask_image ?
    # (0, 0)
    # (0, 1)
    
    for c in range(0, 3): # BGR 채널에 대해서
        image[y-h:y+h, x-w:x+w, c] = (overlay_image[:, :, c] * mask_image) + (image[y-h:y+h, x-w:x+w, c] * (1 - mask_image))

# 얼굴을 찾고, 찾은 얼굴에 표시를 해주기 위한 변수 정의
mp_face_detection = mp.solutions.face_detection # 얼굴 검출을 위한 face_detection 모듈을 사용
mp_drawing = mp.solutions.drawing_utils # 얼굴의 특징을 그리기 위한 drawing_utils 모듈을 사용

# 동영상 파일 열기
cap = cv2.VideoCapture('face_video.mp4')

# 이미지 불러오기
image_right_eye = cv2.imread('right_eye1.png', cv2.IMREAD_UNCHANGED) # 100 x 100
image_left_eye = cv2.imread('left_eye1.png', cv2.IMREAD_UNCHANGED) # 100 x 100
image_nose = cv2.imread('nose1.png', cv2.IMREAD_UNCHANGED) # 300 x 100 (가로, 세로)

with mp_face_detection.FaceDetection(model_selection=0, min_detection_confidence=0.7) as face_detection:
    while cap.isOpened():
        success, image = cap.read()
        if not success:
            break

        # To improve performance, optionally mark the image as not writeable to
        # pass by reference.
        image.flags.writeable = False
        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
        results = face_detection.process(image)

        # Draw the face detection annotations on the image.
        image.flags.writeable = True
        image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)
        if results.detections:
            # 6개의 특징 : 오른쪽 눈, 왼쪽 눈, 코 끝부분, 입 중심, 오른쪽 귀, 왼쪽 귀 (귀구슬점, 이주)
            for detection in results.detections:
                #mp_drawing.draw_detection(image, detection)
                
                # 특정 위치 가져오기
                keypoints = detection.location_data.relative_keypoints
                right_eye = keypoints[0] # 오른쪽 눈
                left_eye = keypoints[1] # 왼쪽 눈
                nose_tip = keypoints[2] # 코 끝부분
                
                h, w, _ = image.shape # height, width, channel : 이미지로부터 세로, 가로 크기 가져옴
                right_eye = (int(right_eye.x * w) - 20, int(right_eye.y * h) - 130) # 이미지 내에서 실제 좌표 (x, y)
                left_eye = (int(left_eye.x * w)+ 20, int(left_eye.y * h) - 130)
                nose_tip = (int(nose_tip.x * w), int(nose_tip.y * h))
                
                # 캐릭터 그리기
                overlay(image, *right_eye, 50, 50, image_right_eye)
                overlay(image, *left_eye, 50, 50, image_left_eye)
                overlay(image, *nose_tip, 150, 50, image_nose)
                
                
        cv2.imshow('MediaPipe Face Detection', cv2.resize(image, None, fx=0.5, fy=0.5))
        if cv2.waitKey(1) == ord('q'):
            break
            
cap.release()
cv2.destroyAllWindows()
```
이미지를 읽어올 때 추가로 옵션을 주지 않으면 알파 값을 가져오지 않아 4채널이 되지 않는다.

1 - mask_image는 mask_image의 값을 반전시키는 효과를 가진다.

완전 투명한 영역은 원본 이미지에서 사용하고, 불투명한 영역은 overlay_image에서 가져다 쓰게 된다.

결과 이미지 (3채널 BGR) = (덮어씌울 이미지 (4채널 BGRA) x mask_image) + (대상 이미지 (3채널 BGR) x (1 - mask_image))

![chara_on_face](https://user-images.githubusercontent.com/38313522/151654993-41c020a1-8046-4809-b075-0a2a7a9784a0.PNG)

# Deep Learning from Scratch
## 1. 시작
### 단순한 그래프 그리기
```python
import numpy as np
import matplotlib.pyplot as plt

# 데이터 준비
x = np.arange(0, 6, 0.1) # 0에서 6까지 0.1 간격으로 생성
y = np.sin(x)

# 그래프 그리기
plt.plot(x, y)
plt.show()
```
![sin_graph](https://user-images.githubusercontent.com/38313522/151655621-10b25b80-252a-4027-b00a-306b7a990ec0.PNG)

### cos 함수를 추가로 표시
```python
import numpy as np
import matplotlib.pyplot as plt

# 데이터 준비
x = np.arange(0, 6, 0.1) # 0에서 6까지 0.1 간격으로 생성
y1 = np.sin(x)
y2 = np.cos(x)

# 그래프 그리기
plt.plot(x, y1, label='sin')
plt.plot(x, y2, linestyle='--', label='cos') # cos 함수는 점선으로 그림
plt.xlabel('x') # x축 이름
plt.ylabel('y') # y축 이름
plt.title('sin & cos')
plt.legend()
plt.show()
```
![sin_and_cos](https://user-images.githubusercontent.com/38313522/151655704-f31fcba6-d9a7-4c16-aebb-db24b50ea4ff.PNG)

### pyplot으로 이미지 표시
```python
import matplotlib.pyplot as plt
from matplotlib.image import imread

img = imread('aurora.jpg') # 이미지 읽어오기

plt.imshow(img)
plt.show()
```
![imshow](https://user-images.githubusercontent.com/38313522/151655892-34e477e7-37de-4b43-8a7e-09074f9904bd.PNG)

## 2. 퍼셉트론 (perceptron)
### 퍼셉트론이란?
퍼셉트론은 다수의 신호를 입력으로 받아 하나의 신호를 출력한다. 퍼셉트론 신호는 '흐른다/안 흐른다(1 or 0)'의 두 가지 값을 가질 수 있다.

![perceptron](https://user-images.githubusercontent.com/38313522/151656004-2fc117b2-f665-4704-aa16-afb2a278d3e4.png)

위의 그림에서 x1과 x2는 입력 신호, y는 출력 신호, w1과 w2는 가중치를 의미한다. 그림에서 원을 뉴런 혹은 노드라고 부른다.   
입력 신호가 뉴런에서 보내질 때는 각각 고유한 가중치가 곱해진다. 이렇게 뉴런에서 보내온 신호의 총합이 정해진 한계를 넘어설 때만 1을 출력한다. 이 한계를 임계값이라고 하고 Θ로 나타낸다.

식으로 나타내면 다음과 같다.

![perceptron2](https://user-images.githubusercontent.com/38313522/151656161-74d28362-1c75-4395-af58-a85e7152f843.PNG)

### 단순한 논리 회로
#### **AND 게이트**
두 입력이 모두 1일 때만 1을 출력하고, 그 외에는 0을 출력한다.
x1|x2|y
:-:|:-:|:-:
0|0|0
1|0|0
0|1|0
1|1|1

위와 같은 조건을 충족하는 퍼셉트론을 만들기 위한 조합은 무수히 많다. x1과 x2 모두가 1일 때만 가중 신호의 총합이 주어진 입계값을 웃돌게하면 된다.  
예를 들면 (w1, w2, Θ)가 (0.5, 0.5, 0.7)일 때, (1.0, 1.0, 1.0)일 때 등이 있다.

#### **NAND 게이트와 OR 게이트**
NAND는 Not AND를 의미하여, AND 게이트의 출력을 뒤집으면 된다.
x1|x2|y
:-:|:-:|:-:
0|0|1
1|0|1
0|1|1
1|1|0

AND 게이트를 구현하는 매개변수의 부호를 모두 반전하기만 하면 NAND 게이트가 된다.  
예로는 (-0.5, -0.5, -0.7) 조합이 있다.

OR 게이트는 입력 신호 중 하나 이상이 1이면 출력이 1이 되는 논리 회로다. 
x1|x2|y
:-:|:-:|:-:
0|0|0
1|0|1
0|1|1
1|1|1

예로는 (1.0, 1.0, 0.9)가 있을 것이다.

퍼셉트론으로 논리 회로를 표현할 수 있고, 퍼셉트론의 구조는 AND, NAND, OR 게이트 모두에서 똑같다. 세 가지 게이트에서 다른 것은 매개변수의 값 뿐이다.
