# Day 110
# Deep Learning from Scratch
## 2. 퍼셉트론 (perceptron)
### 퍼셉트론 구현하기
#### **간단한 구현**
```python
def AND(x1, x2):
    w1, w2, theta = 0.5, 0.5, 0.7
    tmp = (x1 * w1) + (x2 * w2)
    if tmp <= theta:
        return 0
    elif tmp > theta:
        return 1
```
매개변수 w1, w2, theta는 함수 안에서 초기화하고, 가중치를 곱한 입력의 총합이 임계값을 넘으면 1, 그 외에는 0을 반환한다.
```python
print(AND(0, 0)) # 0
print(AND(1, 0)) # 0
print(AND(0, 1)) # 0
print(AND(1, 1)) # 1
```

#### **가중치와 편향 도입**
Θ를 -b로 치환하면 퍼셉트론의 동작이 아래의 식처럼 된다.

![perceptron3](https://user-images.githubusercontent.com/38313522/151692148-4e772de3-7ab6-4b79-85c5-388457b265c0.PNG)

의미는 기존의 식과 같다.  
b는 편향(bias)이라고 한다.

퍼셉트론은 입력 신호에 가중치를 곱한 값과 편향을 합하여, 그 값이 0을 넘으면 1을 출력하고 그렇지 않으면 0을 출력한다.

식을 구현하면 아래와 같다.
```python
import numpy as np
x = np.array([0, 1])     # 입력 신호
w = np.array([0.5, 0.5]) # 가중치
b = -0.7                 # 편향
```
```python
w * x # array([0. , 0.5])
np.sum(w * x) # 0.5
np.sum(w * x) + b # -0.19999999999999996, 약 -0.2 (부동소수점 수에 의한 연산 오차)
```

#### **가중치와 편향 구현하기**
```python
def AND(x1, x2):
    x = np.array([x1, x2])
    w = np.array([0.5, 0.5])
    b = -0.7
    tmp = np.sum(w * x) + b
    if tmp <= 0:
        return 0
    else:
        return 1
```
가중치와 편향을 도입한 AND 게이트이다. 

가중치는 각 입력 신호가 결과에 주는 영향력(중요도)을 조절하는 매개변수고, 편향은 뉴런이 얼마나 쉽게 활성화 하느냐를 조정하는 매개변수이다. 

> 편향은 '한쪽으로 치우쳐 균형을 깬다'라는 의미를 가지고 있다. 그래서 두 입력이 모두 0이어도 결과로 0이 아닌 편향 값을 출력한다.

NAND와 OR 게이트도 구현하면 아래와 같다.
```python
def NAND(x1, x2):
    x = np.array([x1, x2])
    w = np.array([-0.5, -0.5])
    b = 0.7
    tmp = np.sum(w * x) + b
    if tmp <= 0:
        return 0
    else:
        return 1
    
def OR(x1, x2):
    x = np.array([x1, x2])
    w = np.array([1, 1])
    b = -0.9
    tmp = np.sum(w * x) + b
    if tmp <= 0:
        return 0
    else:
        return 1    
```
보면 AND, NAND, OR는 모두 같은 구조로 가중치 매개변수의 값만이 달라지는 것을 확인 가능하다.

### 퍼셉트론의 한계
#### **XOR 게이트**
x1과 x2 중 한쪽이 1일 때만 1을 출력한다. 

x1|x2|y
:-:|:-:|:-:
0|0|0
1|0|1
0|1|1
1|1|0

![OR_XOR](https://user-images.githubusercontent.com/38313522/151692970-fc96deb4-4d79-4994-816e-9c63ec3c2a6f.PNG)

위의 그림과 같이 OR 게이트는 직선으로 동그라미와 세모를 나눌 수 있지만 XOR 게이트는 직선 하나로 나눌 수 없다.

#### **선형과 비선형**
하지만 곡선이라면 동그라미와 세모를 나눌 수 있다.

![XOR](https://user-images.githubusercontent.com/38313522/151693059-17f789ab-bd87-48fa-950d-9a0a75b26217.PNG)


곡선의 영역을 비선형 영역, 직선의 영역을 선형 영역이라고 한다.

### 다중 퍼셉트론
퍼셉트론은 층을 쌓아 다층 퍼셉트론(multi-layer perceptron)을 만들 수 있다.
#### **기존 게이트 조합하기**
XOR 게이트를 만드는 방법은 다양하고, 그중 하나가 AND, NAND, OR 게이트를 조합하는 것이다. 

![XOR_multi_layer](https://user-images.githubusercontent.com/38313522/151693682-52766a7b-6db1-4047-9a22-493275b18b18.PNG)

위와 같이 조합하면 XOR 게이트를 만들 수 있다.

NAND와 OR 게이트에서 나온 출력을 각각 s1, s2로 하면 진리표가 다음과 같이 나온다.

x1|x2|s1|s2|y
:-:|:-:|:-:|:-:|:-:
0|0|1|0|0
1|0|1|1|1
0|1|1|1|1
1|1|0|1|0

#### **XOR 게이트 구현**
```python
def XOR(x1, x2):
    s1 = NAND(x1, x2)
    s2 = OR(x1,x2)
    y = AND(s1, s2)
    return y
```
```python
print(XOR(0, 0)) # 0
print(XOR(1, 0)) # 1
print(XOR(0, 1)) # 1
print(XOR(1, 1)) # 0
```
이 2층 퍼셉트론의 동작은 단순하다.

1. 0층의 두 뉴런이 입력 신호를 받아 1층의 뉴런으로 신호를 보낸다.
2. 1층의 뉴런이 2층의 뉴런으로 신호를 보내고, 2층의 뉴런은 y를 출력한다.

이와 같이 단층 퍼셉트론으로는 표현하지 못한 것을 층을 하나 늘리는 것으로 구현할 수 있었다.

## 3. 신경망
퍼셉트론은 복잡한 함수도 표현할 수 있지만 가중치를 설정하는 작업이 수동이라는 것이다.  
신경망은 가중치 매개변수의 적절한 값을 데이터로부터 자동으로 학습하는 능력이 중요한 성질이다. 

### 퍼셉트론에서 신경망으로
#### **신경망의 예**
![Neural](https://user-images.githubusercontent.com/38313522/151694128-493859b0-c90a-4113-be1c-210834bde73b.png)

가장 왼쪽 줄을 입력층, 맨 오른쪽 줄을 출력층, 중간 줄을 은닉층이라고 한다.

은닉층의 뉴런은 사람 눈에는 보이지 않는다.

#### **퍼셉트론 복습**
편향을 명시하면 아래의 그림과 같이 표현된다.

![perceptron_bias](https://user-images.githubusercontent.com/38313522/151694247-e851102b-c691-4417-9a18-8e95fa5905e8.PNG)

가중치가 b이고 입력이 1인 뉴런이 추가된 것이다. 

이 퍼셉트론은 x1, x2, 1이라는 3개의 신호가 뉴런에 입력되어, 각 신호에 가중치를 곱한 후 다음 뉴런에 전달되고, 다음 뉴런에서는 신호들의 값을 더하여, 그 합이 0을 넘으면 1을 출력하고 그렇지 않으면 0을 출력한다.

편향의 입력 신호는 항상 1이다.

식을 다시 정리하면 아래와 같다.

![perceptron_new](https://user-images.githubusercontent.com/38313522/151694371-049b6623-6030-4f67-a5a9-b3e918a522a9.PNG)

입력 신호의 총합이 h(x)라는 함수를 거쳐 변환되어, 변환된 값이 y의 출력이 되는 것을 보여준다.

#### **활성화 함수의 등장**
위의 h(x) 같이 입력 신호의 총합을 출력 신호로 변환하는 함수를 활성화 함수(activation function)라고 한다. 활성화 함수는 입력 신호의 총합이 활성화를 일으키는지를 정하는 역할을 한다.

<img width="238" alt="e 3 2" src="https://user-images.githubusercontent.com/38313522/151694600-98915a3f-bbc6-4530-8fb2-b0330cb960d9.png">

이 식은 가중치가 곱해진 입력 신호의 총합을 계산하고, 그 합을 활성화 함수에 입력해 결과를 내는 2단계로 처리되어 아래의 2개의 식으로 나눌 수 있다.

<img width="191" alt="e 3 4" src="https://user-images.githubusercontent.com/38313522/151694565-9524c9f7-25fb-4ac8-b087-610b1df4c27d.png">

<img width="90" alt="e 3 5" src="https://user-images.githubusercontent.com/38313522/151694575-11eb9115-9853-401f-9f64-6c45ae3720c8.png">

첫 번째 식은 가중치가 달린 입력 신호와 편향의 총합을 계산하고, 이를 a로 지정하고, 두 번째 식에서 a를 함수 h()에 넣어 y를 출력하는 흐름이다.

그림으로 나타내면 다음과 같다.

<img width="289" alt="fig 3-4" src="https://user-images.githubusercontent.com/38313522/151694660-14417c7b-829b-427d-9706-5f01c0227d1d.png">

### 활성화 함수
임계값을 경계로 출력이 바뀌는 함수를 계단 함수(step function)라고 한다.

#### **시그모이드 함수(sigmoid function)**
<img width="198" alt="e 3 6" src="https://user-images.githubusercontent.com/38313522/151694777-7ca04bf9-751f-4768-bcb8-e238c27a8df7.png">

exp(-x)는 e의 -x승을 뜻한다. 

#### **계단 함수 구현**
단순한 구현
```python
def steop_function(x):
    if x > 0:
        return 1
    else:
        return 0
```
이렇게 단순히 구현하면 인수는 실수만 받아들이고, 넘파이 배열은 넣을 수 없다.

개선하면 이렇게 된다.
```python
def step_function(x):
    y = x > 0
    return y.astype(int)
```

```python
import numpy as np
x = np.array([-1.0, 1.0, 2.0])
y = x > 0
y # array([False, True, True])
```
넘파이 배열에 부등호 연산을 수행하면 배열의 원소 각각에 부등호 연산을 수행한 bool 배열이 생성된다. 여기서 y의 원소를 bool에서 int형으로 바꿔준다.
```python
y = y.astype(int)
y # array([0, 1, 1])
```
bool을 int로 변환하면 True는 1로, False는 0으로 변환된다. 위의 과정을 통해 개선한 것이 위의 함수인 것이다.