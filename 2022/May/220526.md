## 나이브 베이즈의 이해
베이지안 분류기
- 훈련 데이터를 활용해 특징 값이 제공하는 증거를 기반으로 결과가 관측될 확률을 계산
- 결과에 대한 전체 확률을 추정하고자 동시에 여러 속성 정보를 고려해야만 하는 문제에 가장 적합

### 베이지안 기법의 기본 개념
베이지안 확률 이론
- 사건에 대한 우도는 복수 시행에서 즉시 이용할 수 있는 증거를 기반으로 해서 추정해야만 한다.

베이지안 기법
- 관측 데이터에서 사건의 확률을 추정하는 방법에 통찰력을 제공
- 가용한 모든 증거를 활용해 예측을 절묘하게 변경

### 확률의 이해
사건의 확률
- 관측 데이터에서 사건이 발생한 시행 횟수를 전체 시행 횟수로 나눠서 추정
- 이메일이 50개 왔는데 10개가 스팸이면 받은 이메일이 스팸일 확률은 10/50으로 20%가 됨
- 즉 P(스팸) = 0.2이고, P(비스팸) = 0.8로 계산할 수 있다.
- 이 계산은 상호 배타적이고 포괄적인 사건이기 때문에 가능함

### 결합 확률
- P(스팸∩배팅)을 계산하는 것은 두 사건의 결합 확률에 따라 달라짐
- 두 사건이 완전히 관련이 없으면 독립 사건이라고 함
- 모든 사건이 독립이라면 다른 사건을 관측해 어떤 사건을 예측하는 것은 불가능
- 종속 사건이 예측 모델링의 기반
- 합리적인 추정치를 얻으려면 두 사건의 관계에 대한 좀 더 신중한 공식을 사용해야 함 (고급 베이지안 기법 기반 공식)

### 베이즈 정리를 이용한 조건부 확률 계산
P(A|B)
- 조건부 확률
- 사건 B가 발생한 경우 사건 A의 확률
- A의 확률이 사건 B가 발생한 경우에 종속적

공식
- P(A|B) = P(A∩B) / P(B)
- P(A|B) = P(B|A)P(A) / P(B)

설명
- P(A|B)의 최고 추정치는 B가 발생했던 모든 시행에서 A가 B와 함께 발생한 시행의 비율
- 식은 P(A∩B) 값이 B의 발생 확률에 맞춰 조정 됨
- B가 희소하면 P(B)와 P(A∩B)는 항상 작을 것이고, A와 B가 거의 항상 함께 발생하면 P(A|B)는 B의 확률에 관계없이 높을 것임

![01](https://user-images.githubusercontent.com/38313522/170405713-7edb69cb-ea3d-453b-a901-315f5d783e51.PNG)
- 좌측이 빈도표, 우측이 우도표

사전 확률
- 받은 이메일의 내용을 모르는 상태에서 이메일의 스팸 상태를 가장 잘 예측하는 것은 이전 이메일이 스팸일 확률인 P(스팸)
- 이 추정을 사전 확률이라고 함

우도
- 단어 '배팅'이 나타났던 빈도를 관측하고자 이전에 받았던 메일들을 보고 추가 증거를 얻을 수 있었다고 가정
- 이전 스팸 메일에서 단어 '배팅'이 사용된 확률 P(배팅|스팸)을 우도라고 함
- '배팅'이 어떤 메일에라도 나타날 확률 P(배팅)을 주변 우도라고 함
- 기존의 확률을 모른다고 치고, 관측값들로만 확률을 측정한 것

사후 확률
- 이 증거에 베이즈 이론을 적용해 메일이 스팸이 될 확률을 측정한 사후 확률을 계산할 수 있음
- P(스팸|배팅)

### 나이브 베이즈 알고리즘
장점
- 간단하고 빠르고 매우 효율적
- 노이즈와 누락 데이터를 잘 처리함
- 훈련에는 상대적으로 적은 예시가 필요하지만, 대용량의 예시에도 매우 잘 작동
- 예측용 추정 확률을 쉽게 얻을 수 있음

단점
- 모든 특징이 동등하게 중요하고 독립이라는 가정이 잘못된 경우가 자주 있음
- 수치 특징이 많은 데이터셋에는 이상적이지 않음
- 추정된 확률이 예측된 클래스보다 덜 신뢰할 만함

나이브 베이즈 알고리즘
- 데이터셋의 모든 특징이 동등하게 중요하고 독립적이라고 가정
- 이 가정은 대부분의 실제 응용에는 거의 맞지 않음
- 이런 가정이 맞지 않는 대부분의 경우에도 나이브 베이즈가 여전히 잘 작동함
- 즉, 융통성이 있고, 정확함

### 나이브 베이즈를 이용한 분류
![02](https://user-images.githubusercontent.com/38313522/170405848-bbc153ed-a216-4a83-8d66-70726d88f78c.PNG)
- 감시할 단어를 추가한 우도표

예시
- W1과 W4=yes고 W2와 W3=No일 경우 스팸일 확률
- P(spam|W1∩¬W2∩¬W3∩W4) = P(spam|W1∩¬W2∩¬W3∩W4|spam)P(spam) / P(spam|W1∩¬W2∩¬W3∩W4)
- 특징이 추가되면 가능한 모든 사건 교집합에 대해 확률을 저장해야 하므로 엄청난 양의 메모리 필요
- 스팸에 대한 조건부 확률
    - P(spam|W1∩¬W2∩¬W3∩W4) ∝ P(W1|spam)P(¬W2|spam)P(¬W3|spam)P(W4|spam)P(spam)
- 우도표의 값을 이용해 대입
- 스팸의 전체 우도 = (4/20) * (10/20) * (20/20) * (12/20) * (20/100) = 0.012
- 햄의 우도 = (1/80) * (66/80) * (71/80) * (23/0) * (0/100) = 0.002
- 스팸일 확률 = 0.012/(0.012 + 0.002) = 0.857

### 라플라스 추정량
예시
- W11, W2, W3, W4를 모두 포함하는 다른 메일을 받았다고 가정
- 스팸의 우도 = 0, 햄의 우도 = 0.00005
- 스팸의 확률 = 0%, 햄의 확률 = 100%
- 이 예측은 타당하지 않음
- 계속 곱하기 때문에 중간에 0%가 하나라도 있으면 다른 증거를 실질적으로 무효화해버림
- 라플라스 추정량으로 해결

라플라스 추정량
- 빈도표의 각 합계에 작은 숫자를 더하여 확률이 0이 되지 않도록 보장
- 거의 대부분 1로 설정
- 즉 데이터에 각 클래스 특징 조합이 최소 한 번은 나타나도록 보장

라플라스 추정량 적용
- 우도 함수의 각 분자에 라플라스 값 1씩 더하고, 분모의 각 조건부 확률에 4를 더함
- 스팸의 우도 = (5/24) * (11/24) * (1/24) * (13/24) * (20/100) = 0.0004
- 햄의 우도 = 0.0001
- 스팸의 확률 = 80%, 햄의 확률 = 200%
- 이전보다는 타당한 결과

### 나이브 베이즈에서 수치 특성 이용
- 나이브 베이즈는 데이터를 학습하고자 빈도표를 사용하기 때문에 행렬을 구성하는 각 클래스와 특징 값의 조합을 생성하려면 각 특징이 범주형이어야 함
- 수치 특성은 값의 범주가 없음
- 수치 특징을 이산화 하는 것으로 해결

이산화
- bin이라는 범주에 숫자를 넣는 것
    - 훈련 데이터가 대용량일 때 이상적
- 데이터 분포에서 자연스러운 범주나 절단점을 찾고자 데이터를 탐색하는 것
- 특징의 원래 입자가 더 적은 개수의 범주로 줄기 때문에 정보가 손실됨
- bin의 개수가 너무 적으면 중요한 추세가 모호해짐
- 너무 많으면 나이브 베이즈 빈도표의 총계가 작아져 알고리즘의 노이즈 데이터 민감도가 올라갈 수 있음