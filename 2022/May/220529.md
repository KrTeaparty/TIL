## 예제: 나이브 베이즈 알고리즘을 이용한 휴대폰 스팸 필터링

### 데이터 수집
데이터
- http://www.dt.fee.unicamp.br/~tiago/smsspamcollection/
- SMS 스팸 모음을 조정한 데이터 사용
- SMS 메시지의 텍스트와 스팸읹지를 나타내는 레이블이 있음
- 스팸은 무료라는 단어가 자주 나옴
- 햄 메시지는 특정 요일이 잘 언급됨

나이브 베이즈 분류기
- 메시지의 모든 단어가 제공하는 증거를 감안해 스팸과 햄의 확률을 계산할 것

### 데이터 탐색과 준비
데이터 준비
- 텍스트 데이터의 단어와 문장을 컴퓨터가 이해할 수 있는 형식으로 변환해야 함
- 단어의 순서는 무시하고 단순히 단어의 출현 여부를 표시하는 변수를 제공하는 단어 주머니 사용
```r
setwd("c:/Users/HJK/Desktop/lab/MLwR-master/Chapter04")
sms_raw <- read.csv("sms_spam.csv")
str(sms_raw)
```
```
> str(sms_raw)
'data.frame':	5559 obs. of  2 variables:
 $ type: chr  "ham" "ham" "ham" "spam" ...
 $ text: chr  "Hope you are having a good week. Just checking in" "K..give back my thanks." "Am also doing in cbe only. But have to pay." "complimentary 4 STAR Ibiza Holiday or £10,000 cash needs your URGENT collection. 09066364349 NOW from Landline "| __truncated__ ...
```
- type 항목은 원래 범주형 변수이기 때문에 팩터로 변환
```r
sms_raw$type <- factor(sms_raw$type)
str(sms_raw$type)
table(sms_raw$type)
```
```
> str(sms_raw$type)
 Factor w/ 2 levels "ham","spam": 1 1 1 2 2 1 1 1 2 1 ...
> table(sms_raw$type)

 ham spam 
4812  747 
```
- ham이 4812개, spam이 747개로 레이블

### 텍스트 데이터 정리 및 표준화
- SMS 메시지는 단어, 공백, 숫자 구두점으로 구성된 텍스트 문자열
- 이런 복잡한 데이터를 다루려면 많은 것을 고려해야 함
- 이를 위해 tm이라는 텍스트 마이닝 패키지를 사용
- 텍스트 데이터 처리의 첫 단계는 텍스트 문서의 모음인 corpus를 생성하는 것
- 이번 예제에서 corpus는 SMS 메시지 모음
```r
library(tm)
sms_corpus <- VCorpus(VectorSource(sms_raw$text))
print(sms_corpus)
```
```
> print(sms_corpus)
<<VCorpus>>
Metadata:  corpus specific: 0, document level (indexed): 0
Content:  documents: 5559
```
- 훈련 데이터에 5559개의 SMS 메시지 문서가 포함되어 있는 것을 확인
- tm corpus는 복합 리스트이기 때문에 리스트 연산을 사용 가능
```r
inspect(sms_corpus[1:2])
```
```
> inspect(sms_corpus[1:2])
<<VCorpus>>
Metadata:  corpus specific: 0, document level (indexed): 0
Content:  documents: 2

[[1]]
<<PlainTextDocument>>
Metadata:  7
Content:  chars: 49

[[2]]
<<PlainTextDocument>>
Metadata:  7
Content:  chars: 23
```
- inspect()는 결과의 요약을 표시
- 위에서는 1, 2번째 SMS 메시지의 요약을 볼 수 있음
```r
as.character(sms_corpus[[1]])
```
```
> as.character(sms_corpus[[1]])
[1] "Hope you are having a good week. Just checking in"
```
- as.character()는 실제 메시지 텍스트를 표시
- 한개의 메시지를 보려면 리스트의 한 항목에 as.character() 적용
- 이 때는 이중 괄호를 표기해야 함에 주의
```r
lapply(sms_corpus[1:2], as.character)
```
```
> lapply(sms_corpus[1:2], as.character)
$`1`
[1] "Hope you are having a good week. Just checking in"

$`2`
[1] "K..give back my thanks."
```
- lapply()로 여러 항목에 as.character() 적용 가능

- corpus를 분석하려면 메시지를 개별 단어로 나눠야 함
- 이는 tm_map()로 tm corpus 변환을 적용할 수 있는 방법 제공 (매핑)

소문자만 사용하도록 메시지 표준화
- tolower()는 문자열의 소문자 버전을 반환
- 이를 corpus에 적용하려면 tm 래퍼 함수 content_transformer()를 사용하여 tolower()가 corpus에 접근하는데 사용되는 변환 함수로 취급되게 해야 함
```r
sms_corpus_clean <- tm_map(sms_corpus,
                           content_transformer(tolower))
as.character(sms_corpus[[1]])
as.character(sms_corpus_clean[[1]])
```
```
> as.character(sms_corpus[[1]])
[1] "Hope you are having a good week. Just checking in"
> as.character(sms_corpus_clean[[1]])
[1] "hope you are having a good week. just checking in"
```
숫자 제거
- 유용한 정보를 제공하는 숫자가 있을 수도 있으나 대부분 개별 발신자에게 고유한 것일 수 있어 전체 메시지에 유용한 패턴을 제공하지 않을 것
- removeNumbers는 tm에 내장돼 있기 때문에 content_transformer()를 사용하지 않아도 됨
```r
sms_corpus_clean <- tm_map(sms_corpus_clean, removeNumbers)
```
불용어 제거
- to, and, but, or 같은 단어를 불용어라고 함
- tm 패키지에서 제공하는 stopwords() 사용
- stopwords()는 불용어 벡터를 반환
- 아래의 사용에서 stopwords() 대신 제거하고자 하는 단어의 벡터로 대체도 가능
- removeWords()는 tm 패키지에 있는 변환으로 우리는 불용어 목록에 나타나는 단어를 제거할 때 사용
- 마찬가지로 tm_map() 사용
```r
sms_corpus_clean <- tm_map(sms_corpus_clean,
                           removeWords, stopwords())
```
구두점 제거
- removePuctuation() 사용
- 주의할 점은 "hello...world"에서 구두점을 제거하면 "helloworld"가 되어 버림
```r
sms_corpus_clean <- tm_map(sms_corpus_clean, removePunctuation)
```
형태소 분석
- 단어를 어근 형태로 줄임
- SnowballC 패키지의 wordStem() 사용
- 문자 벡터에 대해 어근 형태의 동일한 용어 벡터를 반환
- wordStem()을 전체 corpus에 적용하고자 stemDocument() 사용
```r
library(SnowballC)
wordStem(c("learn","learned","learning","learns"))
sms_corpus_clean <- tm_map(sms_corpus_clean, stemDocument)
```
```
> wordStem(c("learn","learned","learning","learns"))
[1] "learn" "learn" "learn" "learn"
```
추가 여백 제거
- stripWhitespace() 사용
```r
sms_corpus_clean <- tm_map(sms_corpus_clean, stripWhitespace)
```

정리 전후 비교
```r
lapply(sms_corpus[1:3], as.character)
lapply(sms_corpus_clean[1:3], as.character)
```
```
> lapply(sms_corpus[1:3], as.character)
$`1`
[1] "Hope you are having a good week. Just checking in"

$`2`
[1] "K..give back my thanks."

$`3`
[1] "Am also doing in cbe only. But have to pay."

> lapply(sms_corpus_clean[1:3], as.character)
$`1`
[1] "hope good week just check"

$`2`
[1] "kgive back thank"

$`3`
[1] "also cbe pay"
```
### 텍스트 문서를 단어로 나누기
토큰화
- 개별 용어로 나누는 것
- 토큰은 텍스트 문자열의 한 요소, 여기서는 단어
- DocumentTermMatrix() 사용
- corpus를 가져와 문서 용어 행렬(DTM)이라고 하는 데이터 구조를 생성
- 행은 문서, 열은 용어, 셀은 행이 표현하고 있는 문서에 열이 표현하고 있는 단어가 출현하는 횟수
- SMS 메시지의 DTM은 대부분 0으로 채워져 있음
- 이런 행렬을 희소 행렬이라고 함
- 이는 메시지는 최소 하나의 단어를 포함하지만, 특정한 메시지에 나타나는 어떤 한 단어의 확률은 작다는 것
```r
sms_dtm <- DocumentTermMatrix(sms_corpus_clean)

# 원시 corpus에서 바로 DTM 생성 가능
sms_dtm2 <- DocumentTermMatrix(sms_corpus, control=list(
  tolower=T,
  removeNumbers=T,
  stopwords=T,
  removePuctuation=T,
  stemming=T
))

sms_dtm
sms_dtm2
```
```
> sms_dtm
<<DocumentTermMatrix (documents: 5559, terms: 6542)>>
Non-/sparse entries: 42112/36324866
Sparsity           : 100%
Maximal term length: 40
Weighting          : term frequency (tf)
> sms_dtm2
<<DocumentTermMatrix (documents: 5559, terms: 10734)>>
Non-/sparse entries: 45262/59625044
Sparsity           : 100%
Maximal term length: 48
Weighting          : term frequency (tf)
```
- sms_dtm과 sms_dtm2의 용어 개수에 차이가 있는 이유는 사전 처리 단계의 순서의 차이
- DocumentTermMatrix()는 텍스트 문자열을 단어로 분리한 다음에 정리 함수를 적용
- 그에 따라 조금 다른 불용어 제거 함수 사용

### 훈련 및 테스트 데이터셋 생성
- DTM 객체는 데이터 프레임과 비슷하게 동작하여 표준 [ row, col] 연산을 이용 가능
- DTM은 메시지를 행으로, 단어를 열로 저장하기 때문에 행의 특정 구간과 각 행의 전체 열을 요청해야 함
- train, test 데이터 모두 13%정도의 스팸을 포함하고 있어 균등하게 분할된 것을 알 수 있음
```r
sms_dtm_train <- sms_dtm[1:4169, ]
sms_dtm_test <- sms_dtm[4170:5559, ]

sms_train_labels <- sms_raw[1:4169, ]$type
sms_test_labels <- sms_raw[4170:5559, ]$type

prop.table(table(sms_train_labels))
prop.table(table(sms_test_labels))
```
```
> prop.table(table(sms_train_labels))
sms_train_labels
      ham      spam 
0.8647158 0.1352842 
> prop.table(table(sms_test_labels))
sms_test_labels
      ham      spam 
0.8683453 0.1316547 
```
### Wordcloud
모든 SMS 메시지에 나타난 단어 사용
```r
library(wordcloud)
wordcloud(sms_corpus_clean, min.freq=50, random.order=F)
```
![Rplot](https://user-images.githubusercontent.com/38313522/170858047-60f1a35b-0fab-41d8-bfd9-9951777642a2.png)

Spam과 Ham을 구분
- 위의 것이 spam, 아래가 ham
```r
spam <- subset(sms_raw, type=="spam")
ham <- subset(sms_raw, type=="ham")

wordcloud(spam$text, max.words=40, scale=c(3,0.5))
wordcloud(ham$text, max.words=40, scale=c(3, 0.5))
```
![Rplot01](https://user-images.githubusercontent.com/38313522/170858194-994005bf-7296-4f80-a4b5-595693a53b59.png)

![Rplot02](https://user-images.githubusercontent.com/38313522/170858201-6ebf53f4-5b14-4862-a46f-64cc68fa8288.png)

### 자주 사용하는 단어의 지시자 특성 생성
- 모든 특징이 분류에 유용하지는 않음
- 특징의 개수를 줄이기 위해 다섯 개 이하의 메시지에 나타나거나 훈련 데이터에서 약 0.1% 레코드보다 작게 나타나는 단어를 제거
- findFreqTerms()를 사용
- DTM을 받아 최소 횟수만큼 나타나는 단어를 포함하는 문자 벡터를 반환
```r
# sms_dtm_train에서 최소 5번 나타나는 단어들
findFreqTerms(sms_dtm_train, 5)
sms_freq_words <- findFreqTerms(sms_dtm_train, 5)
```
```
> findFreqTerms(sms_dtm_train, 5)
   [1] "£wk"                 "abiola"             
   [3] "abl"                 "abt"                
   [5] "accept"              "access"             
   [7] "account"             "across"             
   [9] "act"                 "activ"              
  [11] "actual"              "add"                
  [13] "address"             "admir"              
  [15] "adult"               "advanc"             
  [17] "aft"                 "afternoon"          
  [19] "age"                 "ago"                
  [21] "aha"                 "ahead"              
  [23] "aight"               "aint"               
  [25] "air"                 "aiyo"               
  [27] "alex"                "almost"             
  [29] "alon"                "alreadi"            
  [31] "alright"             "also"               
  [33] "alway"               "angri"              
  [35] "announc"             "anoth"              
  [37] "answer"              "anymor"             
  [39] "anyon"               "anyth"              
  [41] "anytim"              "anyway"             
  [43] "apart"               "app"                
  [45] "appli"               "appreci"            
  [47] "arcad"               "ard"                
  [49] "area"                "argu"               
  [51] "argument"            "armand"             
  [53] "around"              "arrang"             
  [55] "arriv"               "asap"               
...
 [985] "tick"                "ticket"             
 [987] "til"                 "till"               
 [989] "time"                "tire"               
 [991] "titl"                "tmr"                
 [993] "toclaim"             "today"              
 [995] "togeth"              "told"               
 [997] "tomo"                "tomorrow"           
 [999] "tone"                "tonight"            
 [ reached getOption("max.print") -- omitted 137 entries ]
```
내용 확인
```r
str(sms_freq_words)
```
```
> str(sms_freq_words)
 chr [1:1137] "£wk" "abiola" "abl" "abt" "accept" "access" ...
```
DTM 필터링
- DTM의 특정 부분을 요청하려면 [row, col]을 사용하되 열 이름이 DTM에 포함된 단어를 따른다는 점을 주목
- DTM을 특정 단어로 제약할 때 이를 이용할 수 있음
```r
sms_dtm_freq_train <- sms_dtm_train[, sms_freq_words]
sms_dtm_freq_test <- sms_dtm_test[, sms_freq_words]
```
- 나이브 베이즈 분류기는 대개 범주형 특징으로 된 데이터에 대해 훈련됨
- 희소 행렬의 셀은 숫자이며 메시지에 나타나는 단어의 횟수를 측정하기 때문에 문제가 됨
- 셀의 값을 단어의 출현 여부에 따라 Yes/No로 범주형 변수로 바꿔야 함
```r
convert_counts <- function(x) {
  x <- ifelse(x > 0, "Yes", "No")
}

sms_train <- apply(sms_dtm_freq_train, MARGIN=2, convert_counts)
sms_test <- apply(sms_dtm_freq_test, MARGIN=2, convert_counts)
```
## 데이터로 모델 훈련
```r
library(e1071)
sms_classfier <- naiveBayes(sms_train, sms_train_labels)
```

## 모델 성능 평가
```r
sms_test_pred <- predict(sms_classifier, sms_test)

library(gmodels)
CrossTable(sms_test_pred, sms_test_labels,
           prop.chisq=F, prop.c=F,
           prop.r=F, dnn=c('predicted', 'actual'))
```
```
> CrossTable(sms_test_pred, sms_test_labels,
+            prop.chisq=F, prop.c=F,
+            prop.r=F, dnn=c('predicted', 'actual'))

 
   Cell Contents
|-------------------------|
|                       N |
|         N / Table Total |
|-------------------------|

 
Total Observations in Table:  1390 

 
             | actual 
   predicted |       ham |      spam | Row Total | 
-------------|-----------|-----------|-----------|
         ham |      1201 |        30 |      1231 | 
             |     0.864 |     0.022 |           | 
-------------|-----------|-----------|-----------|
        spam |         6 |       153 |       159 | 
             |     0.004 |     0.110 |           | 
-------------|-----------|-----------|-----------|
Column Total |      1207 |       183 |      1390 | 
-------------|-----------|-----------|-----------|
```
- 1390개 중에서 36개만 부정확하게 분류된 것을 확인
- ham 메시지 1207개 중에서 6개가 스팸으로 잘못 분류
- spam 메시지 183개 중에서 30개가 ham으로 잘못 분류

## 모델 성능 개선
- 모델을 훈련할 때 라플라스 추정량을 설정하지 않았음
```r
sms_classifier2 <- naiveBayes(sms_train, sms_train_labels, laplace=1)
sms_test_pred2 <- predict(sms_classifier2, sms_test)
CrossTable(sms_test_pred2, sms_test_labels,
           prop.chisq=F, prop.c=F, prop.r=F,
           dnn=c('predicted','actual'))
```
```
> CrossTable(sms_test_pred2, sms_test_labels,
+            prop.chisq=F, prop.c=F, prop.r=F,
+            dnn=c('predicted','actual'))

 
   Cell Contents
|-------------------------|
|                       N |
|         N / Table Total |
|-------------------------|

 
Total Observations in Table:  1390 

 
             | actual 
   predicted |       ham |      spam | Row Total | 
-------------|-----------|-----------|-----------|
         ham |      1189 |        16 |      1205 | 
             |     0.855 |     0.012 |           | 
-------------|-----------|-----------|-----------|
        spam |        18 |       167 |       185 | 
             |     0.013 |     0.120 |           | 
-------------|-----------|-----------|-----------|
Column Total |      1207 |       183 |      1390 | 
-------------|-----------|-----------|-----------|
```
- 1390개 중에서 34개를 잘못 분류
- ham을 잘못 분류하는 경우는 늘어났지만 spam을 잘못 분류하는 경우는 줄었음
- 정확도는 97.5%로 꽤 높은 정확도를 보임