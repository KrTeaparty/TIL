# Day 115
# Deep Learning from Scratch
## 4. 신경망 학습
### 학습 알고리즘 구현
신경망에는 적응 가능한 가중치와 편향이 있고, 이 가중치와 편향을 훈련 데이터에 적응하도록 조정하는 과정을 '학습'이라 한다.

신경망 학습 절차
1. 미니배치 : 훈련 데이터 중 일부를 무작위로 가져온다. 이 선별한 데이터를 미니배치라 하고, 미니배치의 손실 함수 값을 줄이는 것이 목표이다.
2. 기울기 산출 : 미니배치의 손실 함수 값을 줄이기 위해 각 가중치 매개변수의 기울기를 구한다. 기울기는 손실 함수의 값을 가장 작게 하는 방향을 제시한다.
3. 매개변수 갱신 : 가중치 매개변수를 기울기 방향으로 아주 조금 갱신한다.
4. 반복 : 1~3을 반복한다.

#### **2층 신경망 클래스 구현**
```python
class TwoLayerNet:
    def __init__(self, input_size, hidden_size, output_size, weight_init_std=0.01):
        # 가중치 초기화
        self.params = {}
        self.params['W1'] = weight_init_std * np.random.randn(input_size, hidden_size)
        self.params['b1'] = np.zeros(hidden_size)
        self.params['W2'] = weight_init_std * np.random.randn(hidden_size, output_size)
        self.params['b2'] = np.zeros(output_size)
        
    def predict(self, x):
        W1, W2 = self.params['W1'], self.params['W2']
        b1, b2 = self.params['b1'], self.params['b2']
        
        a1 = np.dot(x, W1) + b1
        z1 = sigmoid(a1)
        a2 = np.dot(z1, W2) + b2
        y = softmax(a2)
        
        return y
    
    # x : 입력 데이터, t : 정답 레이블
    def loss(self, x, t):
        y = self.predict(x)
        
        return cross_entropy_error(y, t)
    
    def accuracy(self, x, t):
        y = self.predict(x)
        y = np.argmax(y, axis=1)
        t = np.argmax(t, axis=1)
        
        accuracy = np.sum(y == t) / float(x.shape[0])
        return accuracy
    
    # x : 입력 데이터, t : 정답 레이블
    def numerical_gradient(self, x, t):
        loss_W = lambda W: self.loss(x, t)
        
        grads = {}
        grads['W1'] = numerical_gradient(loss_W, self.params['W1'])
        grads['b1'] = numerical_gradient(loss_W, self.params['b1'])
        grads['W2'] = numerical_gradient(loss_W, self.params['W2'])
        grads['b2'] = numerical_gradient(loss_W, self.params['b2'])
        
        return grads
```
TwoLayerNet 클래스가 사용하는 변수
- params
    - 신경망의 매개변수를 보관하는 딕셔너리 변수(인스턴스 변수)
    - params['W1']은 1번째 층의 가중치, prams['b1']은 1번째 층의 편향
    - params['W2']은 2번째 층의 가중치, prams['b2']은 2번째 층의 편향
- grads
    - 기울기를 보관하는 딕셔너리 변수(numerical_gradient() 메소드의 반환값)
    - grads['W1']은 1번째 층의 가중치의 기울기, grads['b1']은 1번째 층의 편향의 기울기
    - grads['W2']은 2번째 층의 가중치의 기울기, grads['b2']은 2번째 층의 편향의 기울기

TwoLayerNet 클래스의 메소드
- \_\_init\_\_(self.input_size, hidden_size, output_size)
    - 초기화를 수행한다.
    - 인수는 순서대로 입력층의 뉴런 수, 은닉층의 뉴런 수, 출력층의 뉴런 수
- predict(self, x)
    - 예측을 수행한다.
    - 인수 x는 이미지 데이터
- loss(self, x, t)
    - 손실 함수의 값을 구한다.
    - 인수 x는 이미지 데이터, t는 정답 레이블
- accuracy(self, x, t)
    - 정확도를 구한다.
- numerical_gradient(self, x, t)
    - 가중치 매개변수의 기울기를 구한다.

params 변수 확인
```python
net = TwoLayerNet(input_size=784, hidden_size=100, output_size=10)
print(net.params['W1'].shape)
print(net.params['b1'].shape)
print(net.params['W2'].shape)
print(net.params['b2'].shape)
```
> (784, 100)\
(100,)\
(100, 10)\
(10,)

예측 처리는 다음과 같이 실행된다.
```python
x = np.random.rand(100, 784) # 더미 입력 데이터(100장 분량)
y = net.predict(x)
```
grads 변수 확인
```python
x = np.random.rand(100, 784) # 더미 입력 데이터(100장 분량)
t = np.random.rand(100, 10)  # 더미 정답 레이블(100장 분량)

grads = net.numerical_gradient(x, t) # 기울기 계산

print(grads['W1'].shape)
print(grads['b1'].shape)
print(grads['W2'].shape)
print(grads['b2'].shape)
```
> (784, 100)\
(100,)\
(100, 10)\
(10,)

초기화 메소드에서 가중치 매개변수도 정규분포를 따르는 난수로, 편향을 0으로 초기화 한다.

#### **미니배치 학습 구현**
```python
from dataset.mnist import load_mnist

(x_train, t_train), (x_test, t_test) = load_mnist(normalize=True, one_hot_label=True)

train_loss_list = []

# 하이퍼파라미터
iters_num = 10000 # 반복 횟수
train_size = x_train.shape[0]
batch_size = 100  # 미니 배치 크기
learning_rate = 0.1
network = TwoLayerNet(input_size=784, hidden_size=50, output_size=10)

for i in range(iters_num):
    # 미니배치 획득
    batch_mask = np.random.choice(train_size, batch_size)
    x_batch = x_train[batch_mask]
    t_batch = t_train[batch_mask]
    
    # 기울기 계산
    grad = network.numerical_gradient(x_batch, t_batch)
    
    # 매개변수 갱신
    for key in ('W1', 'b1', 'W2', 'b2'):
        network.params[key] -= learning_rate * grad[key]
        
    # 학습 경과 기록
    loss = network.loss(x_batch, t_batch)
    train_loss_list.append(loss)

print(train_loss_list[0])
```

미니 배치 크기는 100으로 설정하여 매번 60,000개의 훈련 데이터에서 임의로 100개의 데이터(이미지 데이터와 정답 레이블 데이터)를 추려낸다. 그리고 그 100개의 미니배치를 대상으로 확률적 경사 하강법을 수행해 매개변수를 갱신한다. 반복 횟수는 10,000번으로 설정, 갱신할 때마다 훈련 데이터에 대한 손실 함수를 계산하고, 그 값을 배열에 추가한다.

<img width="683" alt="fig 4-11" src="https://user-images.githubusercontent.com/38313522/152767253-f9b62cda-9a0d-47c8-8733-d8668fd9ea45.png">

좌측은 10,000회 반복까지의 추이, 우측은 1,000회 반복까지의 추이이다.

#### **시험 데이터로 평가**
위의 결과에서 학습을 반복하는 것으로 손실 함수의 값이 서서히 내려가는 것을 확인했다. 즉, 신경망이 잘 학습하고 있다는 것이지만, 오버피팅을 일으키지 않는지 확인하는 과정이 필요하다. 

에폭(epoch)는 학습에서 훈련 데이터를 모두 소진했을 때의 횟수에 해당한다.  
예를 들면 훈련 데이터 10,000개를 100개의 미니배치로 학습할 경우, 확률적 경사 하강법을 100회 반복하면 모든 훈련 데이터를 소진한 것이 되고, 이 경우 100회가 1 에폭이 되는 것이다.

```python
import numpy as np
from dataset.mnist import load_mnist
(x_train, t_train), (x_test, t_test) = load_mnist(normalize=True, one_hot_label=True)

networ = TwoLayerNet(input_size=784, hidden_size=50, output_size=10)

# 하이퍼파라미터
iters_num = 1000 # 반복 횟수
train_size = x_train.shape[0]
batch_size = 100  # 미니 배치 크기
learning_rate = 0.1

train_loss_list = []
train_acc_list = []
test_acc_list = []

# 1 에폭 당 반복 수
iter_per_epoch = max(train_size / batch_size, 1)

for i in range(iters_num):
    # 미니배치 획득
    batch_mask = np.random.choice(train_size, batch_size)
    x_batch = x_train[batch_mask]
    t_batch = t_train[batch_mask]
    
    # 기울기 계산
    grad = network.numerical_gradient(x_batch, t_batch)
    
    # 매개변수 갱신
    for key in ('W1', 'b1', 'W2', 'b2'):
        network.params[key] -= learning_rate * grad[key]
        
    # 학습 경과 기록
    loss = network.loss(x_batch, t_batch)
    train_loss_list.append(loss)
    
    # 1 에폭 당 정확도 계산
    if i % iter_per_epoch == 0:
        train_acc = network.accuracy(x_train, t_train)
        test_acc = network.accuracy(x_test, t_test)
        train_acc_list.append(train_acc)
        test_acc_list.append(test_acc)
        print('train acc, test acc | ' + str(train_acc) + ', ' + str(test_acc))
```
여기서는 1에폭마다 모든 훈련 데이터와 시험 데이터에 대한 정확도를 계산하고, 그 결과를 기록한다. 정확도를 1에폭마다 계산하는 이유는 for문 안에서 매번 계산하기에는 시간이 오래걸리고, 그렇게 자주 기록할 필요가 없기 때문이다.

<img width="569" alt="fig 4-12" src="https://user-images.githubusercontent.com/38313522/152770617-8d7bc51d-52d6-471a-b924-2b7e690acdcc.png">

훈련 데이터에 대한 정확도는 실선, 시험 데이터에 대한 정확도는 점선이다. 학습이 진행될수록 훈련데이터와 시험 데이터를 사용하고 평가한 정확도가 모두 향상되고 있다. 또한 두 정확도에는 차이가 없음을 알 수 있다. 

이를 통해 이 학습에서 오버피팅이 일어나지 않았음을 알 수 있다. 만약 오버피팅이 일어났다면 어느 순간부터 시험 데이터에 대한 정확도가 점차 떨어지기 시작한다. 그 순간에 학습을 중단하는 것을 조기 종료(early stopping)이라고 하며 오버피팅을 예방할 수 있는 방법이다.