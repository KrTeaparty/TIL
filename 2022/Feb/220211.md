# Day 119
# Deep Learning from Scratch
## 6. 학습 관련 기술들
### 가중치의 초깃값
#### **초깃값을 0으로 설정할 경우**
가중치 감소(weight decay)는 오버피팅을 억제해 범용 성능을 높이는 기법으로, 가중치 매개변수의 값이 작아지도록 학습하는 방법이다.

지금까지 가중치의 초깃값은 0.01 * np.random.randn(10, 100)처럼 정규분포에서 생성되는 값을 0.01배 한 작은 값을 사용했다.

초깃값을 모두 0으로 하면(가중치를 균일한 값으로 설정하면) 안되는 이유는 오차역전파법에서 모든 가중치의 값이 똑같이 갱신되기 때문이다. 그러면 가중치들이 갱신을 거쳐도 여전히 같은 값을 유지해서 가중치를 여러 개 갖는 의미를 사라지게 한다. 그래서 초깃값을 무작위로 설정해 가중치의 대칭적인 구조를 무너뜨리는 것이다.

#### **은닉층의 활성화값 분포**
은닉층의 활성화값(활성화 함수의 출력 데이터)의 분포를 관찰하면 중요한 정보를 얻을 수 있다.

가중치의 초깃값에 따라 은닉층 활성화값들의 변화를 보기 위한 실험을 진행한다.

활성화 함수로 시그모이드 함수를 사용하는 5층 신경망에 무작위로 생성한 입력 데이터를 흘리며 각 층의 활성화값 분포를 히스토그램으로 그려서 확인한다.

```python
import numpy as np
import matplotlib.pyplot as plt

def sigmoid(x):
    return 1 / (1 + np.exp(-x))

x = np.random.randn(1000, 100) # 1000개의 데이터
node_num = 100        # 각 은닉층의 노드(뉴런) 수
hidden_layer_size = 5 # 은닉층이 5개
activations = {}      # 이곳에 활성화값을 저장

for i in range(hidden_layer_size):
    if i != 0:
        x = activations[i-1]
        
    # 초깃값을 변경하려면 이곳을 변경
    w = np.random.randn(node_num, node_num) * 1
    
    a = np.dot(x, w)
    
    # 활성화 함수를 바꾸려면 이곳을 변경
    z = sigmoid(a)
    
    activations[i] = z

# 히스토그램 그리기
for i, a in activations.items():
    plt.subplot(1, len(activations), i+1)
    plt.title(str(i+1) + '-layer')
    if i != 0: plt.yticks([], [])
    plt.hist(a.flatten(), 30, range=(0,1))

plt.tight_layout()
plt.show()
```
![hist_fig](https://user-images.githubusercontent.com/38313522/153566124-ce55a131-ac14-431b-a6ee-7eeb643e40c7.PNG)

각 층의 뉴런이 100개씩 있는 5층 신경망이며, 입력 데이터로서 1,000개의 데이터를 정규분포로 무작위로 생성하여 흘린다.  
활성화 함수로 시그모이드 함수를 사용하고, 각 층의 활성화 결과를 activations 변수에 저장한다.  
히스토그램을 보면 각 층의 활성화값이 0과 1에 치우쳐 분포되어 있는 것을 확인할 수 있다. 시그모이드 함수는 그 출력이 0에 가까워지자(또는 1에 가까워지자) 그 미분은 0에 다가간다. 그래서 데이터가 0과 1에 치우쳐 분포하면 역전파의 기울기 값이 점점 작아지다가 사라지는데 이를 기울기 소실(gradient vanishing)이라고 한다.

가중치의 표준편차를 0.01로 변경하여 실험한다.
```python
# 초깃값을 변경하려면 이곳을 변경
w = np.random.randn(node_num, node_num) * 0.01
```
![hist_fig1](https://user-images.githubusercontent.com/38313522/153567287-2ee31f3c-1f2c-47b5-a06a-2937124da1bb.PNG)

이번에는 0.5 부근에 집중되어 기울기 소실은 일어나지 않는다. 하지만 활성화값들이 치우쳐졌다는 것은 다수의 뉴런이 거의 같은 값을 출력하고 있으니 뉴런을 여러 개 둔 의미가 없어진다. 이렇게 활성화 값들이 치우치면 표현력이 제한된다.

Xavier 초깃값은 사비에르 글로로트와 요슈아 벤지오의 논문에서 권장하는 가중치 초깃값이다. 간단히 설명하면 앞 계층의 노드가 n개라면 초깃값을 표준편차가 1/sqrt(n)이 되도록 설정하면 된다는 것이다.

```python
# 초깃값을 변경하려면 이곳을 변경
w = np.random.randn(node_num, node_num) / np.sqrt(node_num)
```
![hist_xavier](https://user-images.githubusercontent.com/38313522/153568339-ea1c8436-1b98-4a4f-81aa-85cd311218e7.PNG)

층이 깊어지면서 형태가 다소 일그러지나, 앞의 방식들보다 넓게 분포되어 있는 것을 볼 수 있다.

이 일그러짐은 sigmoid 함수 대신 tanh 함수(쌍곡선 함수)를 이용하면 개선된다. 활성화 함수용으로는 원점에서 대칭인 함수가 바람직하다고 알려져 있다.

```python
def tanh(x):
    return np.tanh(x)

# 활성화 함수를 바꾸려면 이곳을 변경
z = tanh(a)
```
![hist_tanh](https://user-images.githubusercontent.com/38313522/153569244-263a8808-d62d-41b4-ad8c-e97ac996b6e8.PNG)

#### **ReLU를 사용할 때의 가중치 초깃값**
Xavier 초깃값은 활성화 함수가 선형인 것을 전제로 이끈 결과이다. 

ReLU를 이용할 때는 ReLU에 특화된 초깃값을 이용하는 것을 권장한다. 이 초깃값은 He 초깃값이라고 한다. He 초깃값은 앞 계층의 노드가 n개일 때, 표준편차가 sqrt(2/n)인 정규분포를 사용한다. ReLU는 음의 영역이 0이라서 Xavier보다 더 넓게 분포시키기 위해 2배의 계수가 필요하다고 해석할 수 있다.

표준편차가 0.01인 정규분포를 가중치 초깃값으로 사용한 경우
```python
import numpy as np
import matplotlib.pyplot as plt

def tanh(x):
    return np.tanh(x)

def sigmoid(x):
    return 1 / (1 + np.exp(-x))

def ReLU(x):
    return np.maximum(0, x)

x = np.random.randn(1000, 100) # 1000개의 데이터
node_num = 100        # 각 은닉층의 노드(뉴런) 수
hidden_layer_size = 5 # 은닉층이 5개
activations = {}      # 이곳에 활성화값을 저장

for i in range(hidden_layer_size):
    if i != 0:
        x = activations[i-1]
        
    # 초깃값을 변경하려면 이곳을 변경
    w = np.random.randn(node_num, node_num) * 0.01
    
    a = np.dot(x, w)
    
    # 활성화 함수를 바꾸려면 이곳을 변경
    z = ReLU(a)
    
    activations[i] = z

# 히스토그램 그리기
for i, a in activations.items():
    plt.subplot(1, len(activations), i+1)
    plt.title(str(i+1) + '-layer')
    if i != 0: plt.yticks([], [])
    plt.hist(a.flatten(), 30, range=(0,1))

plt.tight_layout()
plt.show()
```
![relu_std](https://user-images.githubusercontent.com/38313522/153570749-d9b89308-1f57-4bdc-9d65-8e570987584e.PNG)

Xavier 초깃값을 사용한 경우
```python
# 초깃값을 변경하려면 이곳을 변경
w = np.random.randn(node_num, node_num) / np.sqrt(node_num)
# "/ np.sqrt(node_num)"을 "* np.sqrt(1.0 / node_num)"으로 바꿔도 됨
```
![relu_xavier](https://user-images.githubusercontent.com/38313522/153571354-ddf34a4b-1ed1-41a9-9bb8-995f0d7608ce.PNG)

He 초깃값을 사용한 경우
```python
# 초깃값을 변경하려면 이곳을 변경
w = np.random.randn(node_num, node_num) * np.sqrt(2.0 / node_num)
```
![relu_he](https://user-images.githubusercontent.com/38313522/153571660-03f9e013-c614-4e37-a926-010b3d5aae14.PNG)

결과를 보면 std = 0.01일 때의 각 층의 활성화 값들은 아주 작은 값들이다. 이렇게 신경망에 아주 작은 데이터가 흐른다는 것은 역전파 때 가중치의 기울기 역시 작아진다는 것으로 학습이 거의 이뤄지지 않을 것이다.

Xavier 초깃값 결과를 보면 층이 깊어지면서 치우침이 조금씩 커진다. 이렇게 되면 학습할 때 '기울기 소실' 문제를 일으킨다.

He 초깃값은 모든 층에서 균일하게 분포되었고, 층이 깊어져도 분포가 균일하게 유지되어 역전파 때도 적절한 값이 나올 것이다.

결과적으로 ReLU를 활성화 함수로 사용할 때는 He 초깃값을, sigmoid나 tanh 등의 S자 모양 곡선일 때는 Xavier 초깃값이 모범 사례라고 생각하면 된다.

#### **MNIST 데이터셋으로 본 가중치 초깃값 비교**
```python
import numpy as np
import matplotlib.pyplot as plt
from dataset.mnist import load_mnist

# 0. MNIST 데이터 읽기==========
(x_train, t_train), (x_test, t_test) = load_mnist(normalize=True, one_hot_label=True)

train_size = x_train.shape[0]
batch_size = 128
max_iterations = 2000


# 1. 실험용 설정==========
weight_init_types = {'std=0.01': 0.01, 'Xavier': 'sigmoid', 'He': 'relu'}
optimizer = SGD(lr=0.01)

networks = {}
train_loss = {}
for key, weight_type in weight_init_types.items():
    networks[key] = MultiLayerNet(input_size=784, hidden_size_list=[100, 100, 100, 100],
                                  output_size=10, weight_init_std=weight_type)
    train_loss[key] = []


# 2. 훈련 시작==========
for i in range(max_iterations):
    batch_mask = np.random.choice(train_size, batch_size)
    x_batch = x_train[batch_mask]
    t_batch = t_train[batch_mask]
    
    for key in weight_init_types.keys():
        grads = networks[key].gradient(x_batch, t_batch)
        optimizer.update(networks[key].params, grads)
    
        loss = networks[key].loss(x_batch, t_batch)
        train_loss[key].append(loss)
    
    if i % 100 == 0:
        print("===========" + "iteration:" + str(i) + "===========")
        for key in weight_init_types.keys():
            loss = networks[key].loss(x_batch, t_batch)
            print(key + ":" + str(loss))


# 3. 그래프 그리기==========
markers = {'std=0.01': 'o', 'Xavier': 's', 'He': 'D'}
x = np.arange(max_iterations)
for key in weight_init_types.keys():
    plt.plot(x, smooth_curve(train_loss[key]), marker=markers[key], markevery=100, label=key)
plt.xlabel("iterations")
plt.ylabel("loss")
plt.ylim(0, 2.5)
plt.legend()
plt.show()
```
층별 뉴런 수가 100개인 5층 신경망에서 활성화 함수로 ReLU를 사용했다.

![weight_init_compare](https://user-images.githubusercontent.com/38313522/153575244-82a95d8f-ce26-46db-b6d4-d903a0cb2a6d.png)

결과를 보면 std = 0.01일 때는 학습이 전혀 이뤄지지 않고 있다.