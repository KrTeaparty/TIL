# Day 111
# Deep Learning from Scratch
## 3. 신경망
### 활성화 함수
#### **계단 함수의 그래프**
```python
import numpy as np
import matplotlib.pylab as plt

def step_function(x):
    return np.array(x > 0, dtype=int)

x = np.arange(-5.0, 5.0, 0.1) # -5.0 ~ 5.0 전까지 0.1 간격의 numpy 배열 생성
y = step_function(x)
plt.plot(x, y)
plt.ylim(-0.1, 1.1) # y축 범위 지정
plt.show()
```
![step_function](https://user-images.githubusercontent.com/38313522/152306335-4f002e21-62e7-485f-97b8-0e84cc8b7a6f.PNG)

0을 경계로 출력이 0에서 1(또는 1에서 0)로 바뀐다.

#### **시그모이드 함수 구현**
```python
def sigmoid(x):
    return 1 / (1 + np.exp(-x))
```
```python
# 출력
x = np.array([-1.0, 1.0, 2.0])
sigmoid(x) # array([0.26894142, 0.73105858, 0.88079708])

# 브로드캐스트 예제
t = np.array([1.0, 2.0, 3.0])
1.0 + t # array([2., 3., 4.])
1.0 / t # array([1.        , 0.5       , 0.33333333])
```
simoid 함수에서 np.exp(-x)는 exp(-x) 부분에 해당한다. 이때 x가 numpy 배열이어도 올바른 결과가 나온다.

이 함수가 넘파일 배열을 처리할 수 있는 이유는 넘파이의 브로드캐스트 덕분이다. 

시그모이드 함수를 그래프로 그리면 다음과 같다.
```python
x = np.arange(-5.0, 5.0, 0.1)
y = sigmoid(x)
plt.plot(x, y)
plt.ylim(-0.1, 1.1)
plt.show()
```
![sigmoid](https://user-images.githubusercontent.com/38313522/152307989-6300f63b-5167-434e-92ce-51492f2388c6.PNG)

#### **시그모이드 함수와 계단 함수 비교**
```python
import numpy as np
import matplotlib.pyplot as plt

# 데이터 준비
x = np.arange(-5.0, 5.0, 0.1)
y1 = step_function(x)
y2 = sigmoid(x)

# 그래프 그리기
plt.plot(x, y1, label='step')
plt.plot(x, y2, linestyle='--', label='sigmoid') # cos 함수는 점선으로 그림
plt.xlabel('x') # x축 이름
plt.ylabel('y') # y축 이름
plt.ylim(-0.1, 1.1)
plt.title('step & sigmoid')
plt.legend()
plt.show()
```
![step_and_sigmoid](https://user-images.githubusercontent.com/38313522/152310601-7eb83373-cb28-4847-a389-7eba58001d4b.PNG)

시그모이드 함수는 부드러운 곡선이며 입력에 따라 출력이 연속적으로 변화하지만, 계단 함수는 0을 경계로 출력이 갑자기 바뀌어 버린다. 시그모이드 함수의 이 특성이 신경망 학습에서 중요한 역할을 한다.

계단 함수가 0과 1 중 하나의 값만 돌려주는 반면 시그모이드 함수는 실수를 돌려준다. 즉, 퍼셉트론에서는 0 혹은 1이 흘렀지만, 신경망에서는 연속적인 실수가 흐른다.

두 함수는 큰 관점에서 보면 둘은 같은 모양을 하고 있다. 둘 다 입력이 작을 때의 출력은 0에 가깝고(혹은 0이고), 입력이 커지면 출력이 1에 가까워지는(혹은 1이 되는) 구조인 것이다. 즉, 계단 함수와 시그모이드 함수는 입력이 중요하면 큰 값을 출력하고 입력이 중요하지 않으면 작은 값을 출력한다. 또한, 입력이 아무리 작거나 커도 출력은 0에서 1사이이다. 

#### **비선형 함수**
계단 함수와 시그모이드 함수는 모두 비선형 함수이다.

신경망에서는 선형 함수를 이용하면 신경망의 층을 깊게하는 의미가 없어지기 때문에 비선형 함수를 활성화 함수로 사용해야 한다.

선형 함수의 문제는 층을 아무리 깊게 해도 '은닉층이 없는 네트워크'로도 똑같은 기능을 할 수 있다는 데에 있다.

예를 들면 h(x) = cx를 활성화 함수로 한 3층 네트워크를 만들면 y(x) = h(h(h(x)))가 되는데 결국엔 y(x) = c * c * c * x가 되어 y(x) = ax와 똑같은 식인 것이다. 이처럼 선형 함수를 이용해서는 여러 층으로 구성하는 이점을 살릴 수 없다.

#### **ReLU 함수**
ReLU는 입력이 0을 넘으면 그 입력을 그대로 출력하고, 0 이하이면 0을 출력하는 함수이다.

식으로 나타내면 다음과 같다. 

<img width="188" alt="e 3 7" src="https://user-images.githubusercontent.com/38313522/152312199-4f3c52c8-1f14-4f6f-9e1c-9fb412095640.png">

그래프는 다음과 같다.
```python
def relu(x):
    return np.maximum(0, x)

x = np.arange(-5.0, 5.0, 0.1)
y = relu(x)

plt.plot(x, y)
plt.xlim(-6.0, 6.0)
plt.ylim(-1.0, 5.5)
plt.show()
```
numpy의 maximum 함수는 두 입력 중 큰 값을 선택해 반환하는 함수이다.

![relu](https://user-images.githubusercontent.com/38313522/152312624-7a9334e9-7978-4675-9615-30286262f48c.PNG)

### 다차원 배열의 계산
#### **다차원 배열**
1차원 배열
```python
import numpy as np
A = np.array([1, 2, 3, 4])
print(A, np.ndim(A), A.shape, A.shape[0],sep='\t')
# [1 2 3 4]	1	(4,)	4
```
np.ndim() 함수로 배열의 차원 수를, shape 인스턴스 변수로 배열의 형상을 알 수 있다.

2차원 배열
```python
B = np.array([[1, 2], [3, 4], [5, 6]])
print(B, np.ndim(B), B.shape, sep='\t')
'''
[[1 2]
 [3 4]
 [5 6]]	2	(3, 2)
'''
```
2차원 배열에서 배열의 가로 방향을 행, 세로 방향을 열이라고 한다.

#### **행렬의 곱**
<img width="588" alt="fig 3-11" src="https://user-images.githubusercontent.com/38313522/152314238-3334219d-b5ac-4155-a84e-9a6f12750b14.png">

이를 파이썬으로 구현하면 아래와 같이 된다.
```python
A = np.array([[1, 2], [3, 4]])
B = np.array([[5, 6], [7, 8]])
np.dot(A, B)
'''
array([[19, 22],
       [43, 50]])
'''
```
np.dot()은 입력이 1차원 배열이면 벡터를, 2차원 배열이면 행렬 곱을 계산한다. 주의할 점은 np.dot(A, B)와 np.dot(B, A)는 다른 값이 될 수도 있다는 것이다.

2x3 행렬과 3x2 행렬의 곱
```python
A = np.array([[1, 2, 3], [4, 5, 6]])
B = np.array([[1, 2], [3, 4], [5, 6]])
np.dot(A, B)
'''
array([[22, 28],
       [49, 64]])
'''
```
행렬의 곱을 수행할 때는 행렬의 형상에 주의해야 한다. 

행렬 A의 1번째 차원의 원소 수(열 수)와 행렬 B의 0번째 차원의 원소 수(행 수)가 일치해야 한다. 

예를 들어 A가 3x2 행렬이고, B는 2x4 행렬이라면 A의 열의 수가 2, B의 행의 수가 2로 일치하므로 행렬의 곱을 계산할 수 있어 3x4 행렬이 결과로 나온다.

또 다른 예로 A가 2차원 행렬, B가 1차원 배열일 때도 위의 원칙이 적용된다.
```python
A = np.array([[1, 2], [3, 4], [5, 6]])
B = np.array([7, 8])
np.dot(A, B)
# array([23, 53, 83])
```
결과 행렬의 형상은 행렬 A의 행 수, 행렬 B의 열 수가 된다.

#### **신경망에서의 행렬 곱**
<img width="635" alt="fig 3-14" src="https://user-images.githubusercontent.com/38313522/152316186-81361c7d-09cc-4c3f-93a6-06b39dd73a97.png">

넘파이 행렬을 사용해서 위의 신경망을 구현하는데, 이 신경망은 편향과 활성화 함수를 생략하고 가중치만을 갖는다.
```python
X = np.array([1, 2])
W = np.array([[1, 3, 5], [2, 4, 6]])
Y = np.dot(X, W)
print(Y) # [ 5 11 17]
```

### 3층 신경망 구현
<img width="607" alt="fig 3-15" src="https://user-images.githubusercontent.com/38313522/152316761-a5e8c68a-7984-489c-85c4-2254d84f1843.png">

위의 그림의 3층 신경망에서 수행되는 입력부터 출력까지의 처리(순방향 처리)를 구현한다.

#### **표기법**
<img width="542" alt="fig 3-16" src="https://user-images.githubusercontent.com/38313522/152317386-1cfeed7f-dd5d-46b9-b835-ff83d858a3e9.png">

오른쪽 위의 숫자는 n층의 뉴런임을 뜻하는 번호이다. 가중치의 오른쪽 아래의 두 숫자는 다음 층 뉴런과 앞 층 뉴런의 인덱스 번호이다. 

#### **각 층의 신호 전달 구현**
<img width="606" alt="fig 3-17" src="https://user-images.githubusercontent.com/38313522/152317682-23848bcf-ef7a-40d4-9dc5-86cba00bc676.png">

편향을 뜻하는 뉴런이 추가되었는데 편향은 오른쪽 아래 인덱스가 하나 밖에 없다. 이는 앞 층의 편향 뉴런이 하나뿐이기 때문이다.

화살표가 향하는 것을 수식으로 나타내면 다음과 같다.

<img width="249" alt="e 3 8" src="https://user-images.githubusercontent.com/38313522/152318077-9e68f601-2b00-4325-9b11-5488489f66b7.png">

여기서 행렬의 곱을 이용하면 1층의 가중치 부분을 다음과 같이 간소화 할 수 있다.

<img width="190" alt="e 3 9" src="https://user-images.githubusercontent.com/38313522/152318183-da2b0725-ca46-4c84-9537-3880e8eee0d4.png">

각 행렬은 다음과 같다.

![AXBW](https://user-images.githubusercontent.com/38313522/152319264-5c060ad7-e71b-48e5-9ffb-6f025cd33c44.PNG)

넘파이 다차원 배열로 구현하면 아래와 같다. 이때 입력신호, 가중치, 편향을 적당한 값으로 설정했다.
```python
X = np.array([1.0, 0.5])
W1 = np.array([[0.1, 0.3, 0.5], [0.2, 0.4, 0.6]])
B1 = np.array([0.1, 0.2, 0.3])

print(W1.shape) # (2, 3)
print(X.shape)  # (2,)
print(B1.shape) # (3,)

A1 = np.dot(X, W1) + B1
```

이제 1층의 활성화 함수 처리를 그림으로 나타내면 다음과 같다.

<img width="643" alt="fig 3-18" src="https://user-images.githubusercontent.com/38313522/152320058-81da3a9a-b078-4b53-9858-c3e99bb8e878.png">

은닉층에서의 가중치 합(가중 신호와 편향의 총합)을 a로 표기하고 활성화 함수 h()로 변환된 신호를 z로 표기하고, 활성화 함수로 시그모이드 함수를 사용한다.
```python
Z1 = sigmoid(A1)

print(A1) # [0.3 0.7 1.1]
print(Z1) # [0.57444252 0.66818777 0.75026011]
```

1층에서 2층으로의 신호 전달도 비슷하다.

<img width="646" alt="fig 3-19" src="https://user-images.githubusercontent.com/38313522/152320511-d7ac2f77-9df9-4488-8f3a-8b7274d045d0.png">

```python
W2 = np.array([[0.1, 0.4], [0.2, 0.5], [0.3, 0.6]])
B2 = np.array([0.1, 0.2])

print(Z1.shape) # (3,)
print(W2.shape) # (3, 2)
print(B2.shape) # (2,)

A2 = np.dot(Z1, W2) + B2
Z2 = sigmoid(A2)
```
달라진 점은 1층의 출력 Z1이 2층의 입력이 된다는 점 뿐이다.

마지막으로 2층에서 출력층으로의 신호 전달도 앞의 구현과 비슷하지만, 활성화 함수만 달라진다.

<img width="644" alt="fig 3-20" src="https://user-images.githubusercontent.com/38313522/152321271-7bc332d9-344f-4aa7-80ef-9f96e404a251.png">

```python
def identity_function(x):
    return x

W3 = np.array([[0.1, 0.3], [0.2, 0.4]])
B3 = np.array([0.1, 0.2])

A3 = np.dot(Z2, W3) + B3
Y = identity_function(A3) # 혹은 Y = A3
```
항등 함수인 identity_function()을 정의하고, 이를 출력층의 활성화 함수로 이용했다. 항등 함수는 입력을 그대로 출력하는 함수이다. 이번에는 굳이 정의할 필요 없다.

#### **구현 정리**
```python
def init_network():
    network = {}
    network['W1'] = np.array([[0.1, 0.3, 0.5], [0.2, 0.4, 0.6]])
    network['b1'] = np.array([0.1, 0.2, 0.3])
    network['W2'] = np.array([[0.1, 0.4], [0.2, 0.5], [0.3, 0.6]])
    network['b2'] = np.array([0.1, 0.2])
    network['W3'] = np.array([[0.1, 0.3], [0.2, 0.4]])
    network['b3'] = np.array([0.1, 0.2])
    
    return network

def forward(network, x):
    W1, W2, W3 = network['W1'], network['W2'], network['W3']
    b1, b2, b3 = network['b1'], network['b2'], network['b3']
    
    a1 = np.dot(x, W1) + b1
    z1 = sigmoid(a1)
    a2 = np.dot(z1, W2) + b2
    z2 = sigmoid(a2)
    a3 = np.dot(z2, W3) + b3
    y = identity_function(a3)
    
    return y

network = init_network()
x = np.array([1.0, 0.5])
y = forward(network, x)
print(y) # [0.31682708 0.69627909]
```
지금까지 해온 과정은 위와 같이 정리할 수 있다.

init_network() 함수는 가중치와 편향을 초기화하고 이들을 딕셔너리 변수인 network에 저장한다. 이 network 변수에는 각 층에 필요한 매개변수(가중치와 편향)를 저장한다. 

forward() 함수는 입력 신호를 출력으로 변환하는 처리 과정을 모두 구현하고 있다.