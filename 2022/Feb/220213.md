# Day 121
# Deep Learning from Scratch
## 6. 학습 관련 기술들
### 바른 학습을 위해
#### **오버피팅**
오버피팅이 일어나는 경우
- 매개변수가 많고 표현력이 높은 모델
- 훈련 데이터가 적음

MNIST 데이터셋의 훈련 데이터 중 300개만 사용하고, 7층 네트워크를 사용해 네트워크의 복잡성을 높여서 오버피팅을 의도적으로 일으켜서 확인한다. 각 층의 뉴런은 100개, 활성화 함수는 ReLU이다.

```python
import numpy as np
import matplotlib.pyplot as plt
from dataset.mnist import load_mnist
from common.multi_layer_net import MultiLayerNet
from common.optimizer import SGD

(x_train, t_train), (x_test, t_test) = load_mnist(normalize=True)

# 오버피팅을 재현하기 위해 학습 데이터 수를 줄임
x_train = x_train[:300]
t_train = t_train[:300]

# weight decay（가중치 감쇠） 설정 =======================
#weight_decay_lambda = 0 # weight decay를 사용하지 않을 경우
weight_decay_lambda = 0.1
# ====================================================

network = MultiLayerNet(input_size=784, hidden_size_list=[100, 100, 100, 100, 100, 100], output_size=10)
optimizer = SGD(lr=0.01) # 학습률이 0.01인 SGD로 매개변수 갱신

max_epochs = 201
train_size = x_train.shape[0]
batch_size = 100

train_loss_list = []
train_acc_list = []
test_acc_list = []

iter_per_epoch = max(train_size / batch_size, 1)
epoch_cnt = 0

for i in range(1000000000):
    batch_mask = np.random.choice(train_size, batch_size)
    x_batch = x_train[batch_mask]
    t_batch = t_train[batch_mask]

    grads = network.gradient(x_batch, t_batch)
    optimizer.update(network.params, grads)

    if i % iter_per_epoch == 0:
        train_acc = network.accuracy(x_train, t_train)
        test_acc = network.accuracy(x_test, t_test)
        train_acc_list.append(train_acc)
        test_acc_list.append(test_acc)

        print("epoch:" + str(epoch_cnt) + ", train acc:" + str(train_acc) + ", test acc:" + str(test_acc))

        epoch_cnt += 1
        if epoch_cnt >= max_epochs:
            break


# 그래프 그리기==========
markers = {'train': 'o', 'test': 's'}
x = np.arange(max_epochs)
plt.plot(x, train_acc_list, marker='o', label='train', markevery=10)
plt.plot(x, test_acc_list, marker='s', label='test', markevery=10)
plt.xlabel("epochs")
plt.ylabel("accuracy")
plt.ylim(0, 1.0)
plt.legend(loc='lower right')
plt.show()
```
![overfitting_not_decay](https://user-images.githubusercontent.com/38313522/153745541-35c367e7-a999-4067-baec-cbb4d6a4fb53.PNG)


train_acc_list와 test_acc_list에 에폭 단위의 정확도를 저장한다.

훈련 데이터를 사용해서 측정한 정확도는 매우 높지만 시험 데이터에 대해서는 정확도가 큰 차이가 나는 것을 확인할 수 있다.

이렇게 정확도가 크게 차이가 나는 것은 훈련 데이터에만 적응하여 훈련 때 사용하지 않은 시험 데이터에는 제대로 대응하지 못하는 것이다.

#### **가중치 감소(weight decay)**
가중치 감소는 오버피팅 억제용으로 사용하는 방법이다. 학습 과정에서 큰 가중치에 대해서는 그에 상응하는 큰 페널티를 부과하여 오버피팅을 억제하는 방법이다. 이 방법을 사용하는 이유는 오버피팅이 가중치 매개변수의 값이 커서 발생하는 경우가 많기 때문이다.

가중치 감소는 모든 가중치 각각의 손실 함수에 ½λW²을 더한다. 따라서 가중치의 기울기를 구하는 계산에서는 오차역전파법에 따른 결과에 정규화 항을 미분한 λW를 더한다. λ는 정규화 세기를 조절하는 하이퍼 파라미터로 크게 설정할 수록 큰 가중치에 대한 페널티가 커진다.

L2 노름(norm)은 각 원소의 제곱들을 더한 것으로 가중치 W = (w₁, w₂, .., wn)은 L2 노름에서는 sqrt(w₁² + w₂² + ... + wn²)으로 계산한다. L1 노름은 절댓값의 합, L∞ 노름은 Max 노름이라고도 하며, 각 원소의 절댓값 중 가장 큰 것에 해당한다. 

위 코드는 λ = 0.1로 가중치 감소를 적용한다. 실행하면 다음과 같은 결과가 나온다.

![overfitting](https://user-images.githubusercontent.com/38313522/153745030-941a011b-088d-40c1-9f09-6ed36bd098e9.PNG)

전의 결과와 비교하면 차이가 줄었음을 확인할 수 있어 오버피팅이 억제되었다는 것을 알 수 있다.

#### **드롭아웃(Dropout)**
신경망 모델이 복잡해지면 가중치 감소만으로는 대응하기 어려워지는데 이때 흔히 드롭아웃이라는 기법을 사용한다.

드롭아웃은 뉴런을 임의로 삭제하면서 학습하는 방법이다. 훈련 때 은닉층의 뉴런을 무작위로 골라 삭제하고, 삭제된 뉴런은 신호를 전달하지 않게된다. 훈련 때는 데이터를 흘릴 때마다 삭제할 뉴런을 무작위로 선택하고, 시험 때는 모든 뉴런에 신호를 전달하는 것이다. 단, 시험 때는 각 뉴런의 출력에 훈련 때 삭제 안 한 비율을 곱하여 출력한다.

<img width="609" alt="fig 6-22" src="https://user-images.githubusercontent.com/38313522/153746070-ab783386-8888-4e31-835e-827db5fbb490.png">

```python
class Dropout:
    def __init__(self, dropout_ratio=0.5):
        self.dropout_ratio = dropout_ratio
        self.mask = None
        
    def forward(self, x, train_flg=True):
        if train_flg:
            self.mask = np.random.rand(*x.shape) > self.dropout_ratio
            return x * self.mask
        else:
            return x * (1.0 - self.dropout_ratio)
        
    def backward(self, dout):
        return dout * self.mask
```
forward 메소드에서는 훈련 때(train_flg=True일 때)만 잘 계산해두면 시험 때는 단순히 데이터를 흘리기만 하면 된다. 삭제 안 한 비율은 곱하지 않아도 괜찮다.

핵심은 훈련 시에는 순전파 때마다 self.mask에 삭제할 뉴런을 False로 표시한다는 것이다. self.mask는 x와 형상이 같은 배열을 무작위로 생성하고, 그 값이 dropout_ratio보다 큰 원소만 True로 설정한다. 역전파 때의 동작은 ReLU와 같다. 즉, 순전파 때 신호를 통과시키는 뉴런은 역전파 떄도 신호를 그대로 통과시키고, 순전파 때 통과시키지 않은 뉴런은 역전파 때도 신호를 차단한다.

효율적인 구현은 [체이너 프레임워크](http://chainer.org/)의 드롭아웃 구현을 참고

```python
import numpy as np
import matplotlib.pyplot as plt
from dataset.mnist import load_mnist
from common.multi_layer_net_extend import MultiLayerNetExtend
from common.trainer import Trainer

(x_train, t_train), (x_test, t_test) = load_mnist(normalize=True)

# 오버피팅을 재현하기 위해 학습 데이터 수를 줄임
x_train = x_train[:300]
t_train = t_train[:300]

# 드롭아웃 사용 유무와 비울 설정 ========================
use_dropout = True  # 드롭아웃을 쓰지 않을 때는 False
dropout_ratio = 0.2
# ====================================================

network = MultiLayerNetExtend(input_size=784, hidden_size_list=[100, 100, 100, 100, 100, 100],
                              output_size=10, use_dropout=use_dropout, dropout_ration=dropout_ratio)
trainer = Trainer(network, x_train, t_train, x_test, t_test,
                  epochs=301, mini_batch_size=100,
                  optimizer='sgd', optimizer_param={'lr': 0.01}, verbose=True)
trainer.train()

train_acc_list, test_acc_list = trainer.train_acc_list, trainer.test_acc_list

# 그래프 그리기==========
markers = {'train': 'o', 'test': 's'}
x = np.arange(len(train_acc_list))
plt.plot(x, train_acc_list, marker='o', label='train', markevery=10)
plt.plot(x, test_acc_list, marker='s', label='test', markevery=10)
plt.xlabel("epochs")
plt.ylabel("accuracy")
plt.ylim(0, 1.0)
plt.legend(loc='lower right')
plt.show()
```

trainer는 책에서 제공하는 파일이다.
```python
import numpy as np
from common.optimizer import *

class Trainer:
    """신경망 훈련을 대신 해주는 클래스
    """
    def __init__(self, network, x_train, t_train, x_test, t_test,
                 epochs=20, mini_batch_size=100,
                 optimizer='SGD', optimizer_param={'lr':0.01}, 
                 evaluate_sample_num_per_epoch=None, verbose=True):
        self.network = network
        self.verbose = verbose
        self.x_train = x_train
        self.t_train = t_train
        self.x_test = x_test
        self.t_test = t_test
        self.epochs = epochs
        self.batch_size = mini_batch_size
        self.evaluate_sample_num_per_epoch = evaluate_sample_num_per_epoch

        # optimzer
        optimizer_class_dict = {'sgd':SGD, 'momentum':Momentum, 'nesterov':Nesterov,
                                'adagrad':AdaGrad, 'rmsprpo':RMSprop, 'adam':Adam}
        self.optimizer = optimizer_class_dict[optimizer.lower()](**optimizer_param)
        
        self.train_size = x_train.shape[0]
        self.iter_per_epoch = max(self.train_size / mini_batch_size, 1)
        self.max_iter = int(epochs * self.iter_per_epoch)
        self.current_iter = 0
        self.current_epoch = 0
        
        self.train_loss_list = []
        self.train_acc_list = []
        self.test_acc_list = []

    def train_step(self):
        batch_mask = np.random.choice(self.train_size, self.batch_size)
        x_batch = self.x_train[batch_mask]
        t_batch = self.t_train[batch_mask]
        
        grads = self.network.gradient(x_batch, t_batch)
        self.optimizer.update(self.network.params, grads)
        
        loss = self.network.loss(x_batch, t_batch)
        self.train_loss_list.append(loss)
        if self.verbose: print("train loss:" + str(loss))
        
        if self.current_iter % self.iter_per_epoch == 0:
            self.current_epoch += 1
            
            x_train_sample, t_train_sample = self.x_train, self.t_train
            x_test_sample, t_test_sample = self.x_test, self.t_test
            if not self.evaluate_sample_num_per_epoch is None:
                t = self.evaluate_sample_num_per_epoch
                x_train_sample, t_train_sample = self.x_train[:t], self.t_train[:t]
                x_test_sample, t_test_sample = self.x_test[:t], self.t_test[:t]
                
            train_acc = self.network.accuracy(x_train_sample, t_train_sample)
            test_acc = self.network.accuracy(x_test_sample, t_test_sample)
            self.train_acc_list.append(train_acc)
            self.test_acc_list.append(test_acc)

            if self.verbose: print("=== epoch:" + str(self.current_epoch) + ", train acc:" + str(train_acc) + ", test acc:" + str(test_acc) + " ===")
        self.current_iter += 1

    def train(self):
        for i in range(self.max_iter):
            self.train_step()

        test_acc = self.network.accuracy(self.x_test, self.t_test)

        if self.verbose:
            print("=============== Final Test Accuracy ===============")
            print("test acc:" + str(test_acc))
```
실험 코드를 실행하면 다음과 같은 결과가 나온다.

![dropout](https://user-images.githubusercontent.com/38313522/153746666-9a68b7f4-b72b-446e-a12b-272e83502261.PNG)

실험 조건은 앞의 실험과 같다. 드롭아웃을 적용하니 정확도 차이가 줄었고, 훈련 데이터에 대한 정확도가 100%에 도달하지 않게 되었다. 이렇게 드롭아웃을 이용하면 표현력을 높이면서 오버피팅을 억제할 수 있다.

기계학습에서 앙상블 학습(ensemble learning)이라는 개념이 있다. 이는 개별적으로 학습시킨 여러 모델의 출력을 평균 내어 추론하는 방식이다. 예를 들면 같거나 비슷한 구조의 네트워크를 5개 준비하여 따로따로 학습시키고, 시험 때는 그 5개의 출력을 평균 내어 답하는 것이다. 드롭아웃이 학습 때 뉴런을 무작위로 삭제하는 행위를 매번 다른 모델을 학습시키는 것으로 해석할 수 있어 앙상블 학습과 드롭아웃이 밀접하다고 말할 수 있다.

----

### 적절한 하이퍼파라미터 값 찾기
#### **검증 데이터(validation data)**
하이퍼파라미터의 성능을 평가할 때는 시험 데이터를 사용하면 안 된다는 것을 주의해야 한다. 시험 데이터를 사용하여 하이퍼파라미터를 조정하면 하이퍼파라미터 값이 시험 데이터에 오버피팅되기 때문이다.

하이퍼파라미터를 조정할 때는 하이퍼파라미터 전용 확인 데이터가 필요하다. 이 데이터를 검증 데이터라고 한다.

MNIST 데이터셋은 훈련 데이터와 시험 데이터로만 분리되어 있는데 이 경우 사용자가 직접 데이터를 분리해야 한다. 가장 간단한 방법은 훈련 데이터 중 20% 정도를 검증 데이터로 먼저 분리하는 것이다.

```python
def shuffle_dataset(x, t):
    """데이터셋을 뒤섞는다.
    Parameters
    ----------
    x : 훈련 데이터
    t : 정답 레이블
    
    Returns
    -------
    x, t : 뒤섞은 훈련 데이터와 정답 레이블
    """
    permutation = np.random.permutation(x.shape[0])
    x = x[permutation,:] if x.ndim == 2 else x[permutation,:,:,:]
    t = t[permutation]

    return x, t
```
```python
from common.util import shuffle_dataset
(x_train, t_train), (x_test, t_test) = load_mnist()

# 훈련 데이터 뒤섞기
x_train, t_train = shuffle_dataset(x_train, t_train)

# 20%를 검증 데이터로 분할
validation_rate = 0.20
validation_num = int(x_train.shape[0] * validation_rate)

x_val = x_train[:validation_num]
t_val = t_train[:validation_num]
x_train = x_train[validation_num:]
t_train = t_train[validation_num:]
```
데이터셋 안의 데이터가 치우쳐 있을지도 모르니 훈련 데이터를 분리하기 전에 입력 데이터와 정답 레이블을 섞는다.

shuffle_dataset 함수는 np.random.permutation()을 사용한 함수이다. np.random.permutation()은 무작위로 섞인 배열을 만든다.

#### **하이퍼파라미터 최적화**
하이퍼파라미터 최적화의 핵심은 하이퍼파라미터의 '최적 값'이 존재하는 범위를 조금씩 줄여가는 것이다. 이렇게 하려면 우선 대략적인 범위를 설정하고 그 범위에서 무작위로 하이퍼 파라미터 값을 골라낸(샘플링) 후, 그 값으로 정확도를 평가하고, 이 작업을 여러 번 반복하며 범위를 좁혀가는 것이다.

이 최적화에서 그리드 서치(grid search) 같은 규칙적인 탐색보다 무작위로 샘플링해 탐색하는 편이 좋은 결과를 낸다고 알려져 있는데 이는 최종 정확도에 미치는 영향력이 하이퍼파라미터마다 다르기 때문이다.

하이퍼파라미터의 범위는 대략적으로 지정하는 것이 효과적이다. 0.001에서 1,000 사이와 같이 10의 거듭제곱 단위로 범위를 지정하는데 이를 로그 스케일(log scale)로 지정한다고 한다.

최적화할 때는 오랜시간이 걸리므로 나쁠 듯한 값은 일찍 포기하는 것이 좋고, 학습을 위한 에폭을 작게 하여, 1회 평가에 걸리는 시간을 단축하는 것이 효과적이다.

단계를 정리하면 다음과 같다.
- 0단계 : 하이퍼 파라미터 값의 범위를 설정한다.
- 1단계 : 설정된 범위에서 하이퍼파라미터의 값을 무작위로 추출한다.
- 2단계 : 1단계에서 샘플링한 하이퍼파라미터 값을 사용하여 학습하고, 검증 데이터로 정확도를 평가한다(단, 에폭은 작게 설정한다).
- 3단계 : 1단계와 2단계를 특정 횟수(100회 등) 반복하며, 그 정확도의 결과를 보고 하이퍼파라미터의 범위를 좁힌다.

이 단계를 반복하여 범위를 좁혀가다가 어느 정도 좁아지면 그 압축한 범위에서 값을 하나 골라내는 것이 최적화하는 하나의 방법이다.

이 외에는 베이즈 최적화(Bayesian optimization)이 있다. 베이즈 최적화는 베이즈 정리(Bayes' theorem)를 중심으로 한 수학 이론을 구사하여 더 엄밀하고 효율적으로 최적화를 수행한다.

#### **하이퍼파라미터 최적화 구현**
MNIST 데이터셋을 사용하여 하이퍼파라미터를 최적화한다.

학습률과 가중치 감소의 세기를 조절하는 계수(가중치 감소 계수)를 탐색하는 문제를 푼다.

0.001 ~ 1,000 사이 같은 로그 스케일 범위에서 무작위로 추출하는 것을 파이썬 코드로 다음과 같이 작성할 수 있다.
> 10 ** np.random.uniform(-3, 3)

이 예에서는 가중치 감소 계수를 10의 -8승 부터 -4승까지, 학습률을 10의 -6승부터 -2승 범위부터 시작한다.

```python
import numpy as np
import matplotlib.pyplot as plt
from dataset.mnist import load_mnist
from common.multi_layer_net import MultiLayerNet
from common.util import shuffle_dataset
from common.trainer import Trainer

(x_train, t_train), (x_test, t_test) = load_mnist(normalize=True)

# 결과를 빠르게 얻기 위해 훈련 데이터를 줄임
x_train = x_train[:500]
t_train = t_train[:500]

# 20%를 검증 데이터로 분할
validation_rate = 0.20
validation_num = int(x_train.shape[0] * validation_rate)
x_train, t_train = shuffle_dataset(x_train, t_train)
x_val = x_train[:validation_num]
t_val = t_train[:validation_num]
x_train = x_train[validation_num:]
t_train = t_train[validation_num:]


def __train(lr, weight_decay, epocs=50):
    network = MultiLayerNet(input_size=784, hidden_size_list=[100, 100, 100, 100, 100, 100],
                            output_size=10, weight_decay_lambda=weight_decay)
    trainer = Trainer(network, x_train, t_train, x_val, t_val,
                      epochs=epocs, mini_batch_size=100,
                      optimizer='sgd', optimizer_param={'lr': lr}, verbose=False)
    trainer.train()

    return trainer.test_acc_list, trainer.train_acc_list


# 하이퍼파라미터 무작위 탐색======================================
optimization_trial = 100
results_val = {}
results_train = {}
for _ in range(optimization_trial):
    # 탐색한 하이퍼파라미터의 범위 지정===============
    weight_decay = 10 ** np.random.uniform(-8, -4)
    lr = 10 ** np.random.uniform(-6, -2)
    # ================================================

    val_acc_list, train_acc_list = __train(lr, weight_decay)
    print("val acc:" + str(val_acc_list[-1]) + " | lr:" + str(lr) + ", weight decay:" + str(weight_decay))
    key = "lr:" + str(lr) + ", weight decay:" + str(weight_decay)
    results_val[key] = val_acc_list
    results_train[key] = train_acc_list

# 그래프 그리기========================================================
print("=========== Hyper-Parameter Optimization Result ===========")
graph_draw_num = 20
cur_graph_num = 0
col_num = 5
row_num = int(np.ceil(graph_draw_num / col_num))
i, j = 0, 0
f, axs = plt.subplots(row_num, col_num, figsize=(15, 15))
f.tight_layout()

for key, val_acc_list in sorted(results_val.items(), key=lambda x:x[1][-1], reverse=True):
    print("Best-" + str(i+1) + "(val acc:" + str(val_acc_list[-1]) + ") | " + key)
    index = i % 5
    if index == 0 and i != 0:
        j += 1

    axs[j][index].set_title("Best-" + str(i+1))
    axs[j][index].set_ylim(0.0, 1.0)
    if i % 5: axs[j][index].set_yticks([])
    axs[j][index].set_xticks([])
    x = np.arange(len(val_acc_list))
    axs[j][index].plot(x, val_acc_list)
    axs[j][index].plot(x, results_train[key], "--")
    i += 1

    if i >= graph_draw_num:
        break

plt.show()
```
>=========== Hyper-Parameter Optimization Result ===========\
Best-1(val acc:0.78) | lr:0.007433901206455341, weight decay:2.600513031150083e-05\
Best-2(val acc:0.77) | lr:0.009013194130712462, weight decay:5.380294000442538e-07\
Best-3(val acc:0.75) | lr:0.006324129641047086, weight decay:2.818655935846745e-06\
Best-4(val acc:0.74) | lr:0.007346858519728556, weight decay:7.855243633410557e-07\
Best-5(val acc:0.73) | lr:0.009939483027891521, weight decay:1.7426540926544947e-07\
Best-6(val acc:0.72) | lr:0.006123256509470679, weight decay:7.726205848599579e-08\
Best-7(val acc:0.69) | lr:0.005948987450378263, weight decay:1.216425786853472e-05\
Best-8(val acc:0.66) | lr:0.004886846074217082, weight decay:1.1753703673986511e-07\
Best-9(val acc:0.61) | lr:0.0037046831124123093, weight decay:7.909086351962598e-07\
Best-10(val acc:0.59) | lr:0.004302774238756059, weight decay:5.878183113949265e-05\
Best-11(val acc:0.59) | lr:0.005316546755578209, weight decay:7.433274531249061e-08\
Best-12(val acc:0.57) | lr:0.004328620012251767, weight decay:2.0850998937089727e-07\
Best-13(val acc:0.57) | lr:0.00394449454801212, weight decay:7.995825573921446e-08\
Best-14(val acc:0.56) | lr:0.006234146323440894, weight decay:1.459381620480323e-08\
Best-15(val acc:0.35) | lr:0.0022213499541120087, weight decay:6.413473974279569e-08\
Best-16(val acc:0.33) | lr:0.0020271194387551953, weight decay:2.752964922370407e-07\
Best-17(val acc:0.32) | lr:0.0019523933009751404, weight decay:1.1083434384334697e-05\
Best-18(val acc:0.3) | lr:0.00142973800014508, weight decay:1.0499703738896335e-06\
Best-19(val acc:0.3) | lr:0.0013963556344734594, weight decay:5.8547067285915636e-05\
Best-20(val acc:0.29) | lr:0.001544107561077589, weight decay:2.0824778780109453e-06

![hyperparam_optimization](https://user-images.githubusercontent.com/38313522/153748952-bde854b5-6d91-4e01-9450-e2cc83f90cb5.png)

검증 데이터의 학습 추이를 정확도가 높은 순서로 나열한 것이다. 실선은 검증 데이터에 대한 정확도, 점선은 훈련 데이터에 대한 정확도이다. Best-5까지의 하이퍼파라미터(학습률과 가중치 감소 계수)를 확인해보면 다음과 같다.
> Best-1(val acc:0.78) | lr:0.007433901206455341, weight decay:2.600513031150083e-05\
Best-2(val acc:0.77) | lr:0.009013194130712462, weight decay:5.380294000442538e-07\
Best-3(val acc:0.75) | lr:0.006324129641047086, weight decay:2.818655935846745e-06\
Best-4(val acc:0.74) | lr:0.007346858519728556, weight decay:7.855243633410557e-07\
Best-5(val acc:0.73) | lr:0.009939483027891521, weight decay:1.7426540926544947e-07

학습률은 0.001 ~ 0.01, 가중치 감소 계수는 10의 -8승에서 -5승정도로 생각할 수 있을 것이다.

이렇게 잘될 것 같은 값의 범위를 관찰하고 범위를 좁혀간다. 그 다음 그 축소된 범위로 똑같은 작업을 반복한다. 이렇게 적절한 값이 위치한 범위를 좁혀가다가 특정 단계에서 최종 하이퍼파라미터 값을 하나 선택한다.