# Day 120
# Deep Learning from Scratch
## 6. 학습 관련 기술들
### 배치 정규화
#### **배치 정규화 알고리즘**
배치 정규화 장점
- 학습 속도 개선
- 초깃값에 크게 의존하지 않는다.
- 오버피팅을 억제한다.

배치 정규화의 기본 아이디어는 각 층에서의 활성화값이 적당히 분포되도록 조정하는 것이다.

중간에 데이터 분포를 정규화하는 '배치 정규화(Batch Norm) 계층'을 신경망에 삽입하여 사용한다.

<img width="647" alt="fig 6-16" src="https://user-images.githubusercontent.com/38313522/153703748-739d939d-c66f-45cd-b35d-dd418da208ff.png">

배치 정규화는 학습 시 미니배치를 단위로 정규화한다. 데이터 분포가 평균이 0, 분산이 1이 되도록 정규화하고, 수식은 다음과 같다.

<img width="207" alt="e 6 7" src="https://user-images.githubusercontent.com/38313522/153703795-95e425fb-9711-462d-bb1e-78a2fae3d77f.png">

미니배치 B = {x1, x2, ..., xm}이라는 m개의 입력 데이터의 집합에 대해 평균 ηB와 분산 σB**2을 구한다. 그리고 입력 데이터를 평균이 0, 분산이 1이 되게 정규화한다. 그리고 ε 기호는 작은 값(10e-7 같은 값)으로, 0으로 나누는 사태를 예방하는 역할을 한다.

배치 정규화 계층마다 이 정규화된 데이터에 고유한 확대(scale)와 이동(shift) 변환을 수행하며, 수식은 다음과 같다.

<img width="123" alt="e 6 8" src="https://user-images.githubusercontent.com/38313522/153703956-f19108ed-8a8d-425d-a3d6-91c322301d2a.png">

γ가 확대를, β가 이동을 담당한다. 두 값은 처음에는 γ = 1, β = 0부터 시작해 학습하면서 적합한 값으로 조정해간다.

배치 정규화 알고리즘은 신경망에서 순전파 때 적용되며 계산그래프는 다음과 같다.

<img width="799" alt="fig 6-17" src="https://user-images.githubusercontent.com/38313522/153704064-1cc7104a-9111-4f6a-beaf-9b2676a71952.png">

#### **배치 정규화의 효과**
```python
import numpy as np
import matplotlib.pyplot as plt
from dataset.mnist import load_mnist
from common.multi_layer_net_extend import MultiLayerNetExtend
from common.optimizer import SGD, Adam

(x_train, t_train), (x_test, t_test) = load_mnist(normalize=True)

train_size = x_train.shape[0]

max_epochs = 20
train_size = x_train.shape[0]
batch_size = 100
learning_rate = 0.01


def __train(weight_init_std):
    bn_network = MultiLayerNetExtend(input_size=784, hidden_size_list=[100, 100, 100, 100, 100], output_size=10, 
                                    weight_init_std=weight_init_std, use_batchnorm=True)
    network = MultiLayerNetExtend(input_size=784, hidden_size_list=[100, 100, 100, 100, 100], output_size=10,
                                weight_init_std=weight_init_std)
    optimizer = SGD(lr=learning_rate)
    
    train_acc_list = []
    bn_train_acc_list = []
    
    iter_per_epoch = max(train_size / batch_size, 1)
    epoch_cnt = 0
    
    for i in range(1000000000):
        batch_mask = np.random.choice(train_size, batch_size)
        x_batch = x_train[batch_mask]
        t_batch = t_train[batch_mask]
    
        for _network in (bn_network, network):
            grads = _network.gradient(x_batch, t_batch)
            optimizer.update(_network.params, grads)
    
        if i % iter_per_epoch == 0:
            train_acc = network.accuracy(x_train, t_train)
            bn_train_acc = bn_network.accuracy(x_train, t_train)
            train_acc_list.append(train_acc)
            bn_train_acc_list.append(bn_train_acc)
    
            print("epoch:" + str(epoch_cnt) + " | " + str(train_acc) + " - " + str(bn_train_acc))
    
            epoch_cnt += 1
            if epoch_cnt >= max_epochs:
                break
                
    return train_acc_list, bn_train_acc_list


# 그래프 그리기==========
x = np.arange(max_epochs)

train_acc_list, bn_train_acc_list = __train(w)

plt.plot(x, bn_train_acc_list, label='Batch Normalization', markevery=2)
plt.plot(x, train_acc_list, linestyle = "--", label='Normal(without BatchNorm)', markevery=2)

plt.ylim(0, 1.0)
plt.xlim(0, 20.0)
plt.ylabel("accuracy")

plt.xlabel("epochs")
plt.legend(loc='lower right')
    
plt.show()
```
중간에 MultiLayerNetExtend랑 몇 가지 못보던 것이 추가된 것은 실험을 위해 책에서 제공하는 것을 가져온 것이다.

![batch_norm](https://user-images.githubusercontent.com/38313522/153705941-610cf3f2-ce21-4a4f-87e0-eb39999e333a.PNG)

결과를 보면 배치 정규화가 학습을 빠르게 진전시키고 있음을 알 수 있다.

가중치 초깃값의 표준편차를 다양하게 바꿔가며 학습 경과를 관찰해보자.
```python
(x_train, t_train), (x_test, t_test) = load_mnist(normalize=True)

# 학습 데이터를 줄임
x_train = x_train[:1000]
t_train = t_train[:1000]

max_epochs = 20
train_size = x_train.shape[0]
batch_size = 100
learning_rate = 0.01


def __train(weight_init_std):
    bn_network = MultiLayerNetExtend(input_size=784, hidden_size_list=[100, 100, 100, 100, 100], output_size=10, 
                                    weight_init_std=weight_init_std, use_batchnorm=True)
    network = MultiLayerNetExtend(input_size=784, hidden_size_list=[100, 100, 100, 100, 100], output_size=10,
                                weight_init_std=weight_init_std)
    optimizer = SGD(lr=learning_rate)
    
    train_acc_list = []
    bn_train_acc_list = []
    
    iter_per_epoch = max(train_size / batch_size, 1)
    epoch_cnt = 0
    
    for i in range(1000000000):
        batch_mask = np.random.choice(train_size, batch_size)
        x_batch = x_train[batch_mask]
        t_batch = t_train[batch_mask]
    
        for _network in (bn_network, network):
            grads = _network.gradient(x_batch, t_batch)
            optimizer.update(_network.params, grads)
    
        if i % iter_per_epoch == 0:
            train_acc = network.accuracy(x_train, t_train)
            bn_train_acc = bn_network.accuracy(x_train, t_train)
            train_acc_list.append(train_acc)
            bn_train_acc_list.append(bn_train_acc)
    
            #print("epoch:" + str(epoch_cnt) + " | " + str(train_acc) + " - " + str(bn_train_acc))
    
            epoch_cnt += 1
            if epoch_cnt >= max_epochs:
                break
                
    return train_acc_list, bn_train_acc_list


# 그래프 그리기==========
weight_scale_list = np.logspace(0, -4, num=16)
x = np.arange(max_epochs)
f, axs = plt.subplots(4, 4, figsize=(15, 15), constrained_layout=True)
j = 0
for i, w in enumerate(weight_scale_list):
    index = i % 4
    #print( "============== " + str(i+1) + "/16" + " ==============")    
    if index == 0 and i != 0:
        j += 1
    train_acc_list, bn_train_acc_list = __train(w)
    
    axs[index][j].set_title("W:" + str(w))
    axs[index][j].set_ylim([0, 1.0])
    axs[index][j].set_xlim([0, 20.0])
    if i == 15:
        plt.plot(x, bn_train_acc_list, label='Batch Normalization', markevery=2)
        plt.plot(x, train_acc_list, linestyle = "--", label='Normal(without BatchNorm)', markevery=2)
    else:
        axs[index][j].plot(x, bn_train_acc_list, markevery=2)
        axs[index][j].plot(x, train_acc_list, linestyle="--", markevery=2)

    plt.ylim(0, 1.0)
    if j % 4:
        axs[index][j].set_yticks([])
    else:
        axs[index][j].set_ylabel("accuracy")
    if index != 3:
        axs[index][j].set_xticks([])
    else:
        axs[index][j].set_xlabel("epochs")
    
plt.savefig('./output/batch_norm_weight.png')
plt.show()
```
![batch_norm_weight](https://user-images.githubusercontent.com/38313522/153710758-c77a81fd-5df5-41ba-9f7d-ffdcef2fb715.png)

실선이 배치 정규화를 사용한 경우, 점선이 사용하지 않은 경과이고, 가중치 초깃값의 표준편차는 각 그래프 위에 표기했다.

결과를 보면 거의 모든 경우에서 배치 정규화를 사용할 때의 학습 진도가 빠른 것으로 나타난다. 배치 정규화를 사용하지 않았을 때 초깃값이 잘 분포되어 있지 않으면 학습이 전혀 진행되지 않는 경우도 확인할 수 있다.