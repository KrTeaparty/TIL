# Day 118
# Deep Learning from Scratch
## 5. 오차역전파법(backpropagation)
### 오차역전파법 구현
#### **오차역전파법을 적용한 신경망 구현**
2층 신경망 클래스 TwoLayerNet 클래스의 인스턴스 변수
- params
    - 딕셔너리 변수, 신경망의 매개변수 보관
    - params['W1']은 1번째 층의 가중치, params['b1']은 1번째 층의 편향
    - params['W2']은 2번째 층의 가중치, params['b2']은 2번째 층의 편향
- layers
    - 순서가 있는 딕셔너리 변수, 신경망의 계층을 보관
    - layers['Affine1'], layer['Relu1'], layers['Affine2']와 같이 각 계층을 순서대로 유지
- lastLayer
    - 신경망의 마지막 계층
    - 이 예에서는 SoftmaxWithLoss 계층

TwoLayerNet 클래스의 메소드
- \_\_init\_\_(self, input_size, hidden_size, output_size, weight_init_std)
    - 초기화 수행
    - 인수는 순서대로 입력층 뉴런 수, 은닉층 뉴런 수, 가중치 초기화 시 정규분포의 스케일
- predict(self, x)
    - 예측을 수행
    - 인수 x는 이미지 데이터
- loss(self, x, t)
    - 손실 함수의 값을 구한다.
    - 인수 x는 이미지 데이턴, t는 정답 레이블
- accuracy(self, x, t)
    - 정확도를 구한다.
- numerical_gradient(self, x, t)
    - 가중치 매개변수의 기울기를 수치 미분 방식으로 구한다.(앞 장과 동일)
- gradient(self, x, t)
    - 가중치 매개변수의 기울기를 오차역전파법으로 구한다.

```python
import numpy as np
from collections import OrderedDict

class TwoLayerNet:
    def __init__(self, input_size, hidden_size, output_size, weight_init_std=0.01):
        # 가중치 초기화
        self.params = {}
        self.params['W1'] = weight_init_std * np.random.randn(input_size, hidden_size)
        self.params['b1'] = np.zeros(hidden_size)
        self.params['W2'] = weight_init_std * np.random.randn(hidden_size, output_size)
        self.params['b2'] = np.zeros(output_size)
        
        #계층 생성
        self.layers = OrderedDict()
        self.layers['Affine1'] = Affine(self.params['W1'], self.params['b1'])
        self.layers['Relu1'] = Relu()
        self.layers['Affine2'] = Affine(self.params['W2'], self.params['b2'])
        
        self.lastLayer = SoftmaxWithLoss()
        
    def predict(self, x):
        for layer in self.layers.values():
            x = layer.forward(x)
            
        return x
    
    # x : 입력 데이터, t : 정답 레이블
    def loss(self, x, t):
        y = self.predict(x)
        return self.lastLayer.forward(y, t)
    
    def accuracy(self, x, t):
        y = self.predict(x)
        y = np.argmax(y, axis=1)
        if t.ndim != 1 : t = np.argmax(t, axis=1)
            
        accuracy = np.sum(y == t) / float(x.shape[0])
        return accuracy
    
    def numerical_gradient(self, x, t):
        loss_W = lambda W: self.loss(x, t)
        
        grads = {}
        grads['W1'] = numerical_gradient(loss_W, self.params['W1'])
        grads['b1'] = numerical_gradient(loss_W, self.params['b1'])
        grads['W2'] = numerical_gradient(loss_W, self.params['W2'])
        grads['b2'] = numerical_gradient(loss_W, self.params['b2'])
        return grads
    
    def gradient(self, x, t):
        # 순전파
        self.loss(x, t)
        
        # 역전파
        dout = 1
        dout = self.lastLayer.backward(dout)
        
        layers = list(self.layers.values())
        layers.reverse()
        for layer in layers:
            dout = layer.backward(dout)
            
        # 결과 저장
        grads = {}
        grads['W1'] = self.layers['Affine1'].dW
        grads['b1'] = self.layers['Affine1'].db
        grads['W2'] = self.layers['Affine2'].dW
        grads['b2'] = self.layers['Affine2'].db
        
        return grads
```
신경망 계층을 OrderedDict에 보관하는데 OrderedDict는 순서가 있는 딕셔너리로, 딕셔너리에 추가한 순서를 기억한다. 그래서 순전파 때는 추가한 순서대로 각 계층의 forward() 메소드를 호출하고, 역전파 때는 계층을 반대 순서로 호출하면 된다.

#### **오차역전파법으로 구한 기울기 검증**
수치 미분은 느려서 오차역전파법이 제대로 구현되면 필요가 없다. 하지만 구현이 쉬워 버그가 숨기 어려워 수치 미분의 결과와 오차역전파법의 결과를 비교하여 오차역전파법을 제대로 구현했는지 검증하는데 사용한다.  
이 방법을 기울기 확인(gradient check)이라고 한다.

```python
from dataset.mnist import load_mnist

# 데이터 읽기
(x_train, t_train), (x_test, t_test) = load_mnist(normalize=True, one_hot_label=True)

network = TwoLayerNet(input_size=784, hidden_size=50, output_size=10)

x_batch = x_train[:3]
t_batch = t_train[:3]

grad_numerical = network.numerical_gradient(x_batch, t_batch)
grad_backprop = network.gradient(x_batch, t_batch)

# 각 가중치의 차이의 절댓값을 구한 후, 그 절댓값들의 평균을 낸다.
for key in grad_numerical.keys():
    diff = np.average(np.abs(grad_backprop[key] - grad_numerical[key]))
    print(key + ':' + str(diff))
```
> W1:4.5964650462033103e-10\
b1:2.7435504021649635e-09\
W2:5.351475167516889e-09\
b2:1.4050077618704693e-07

먼저 MNIST 데이터셋을 읽고 훈련 데이터 일부를 수치 미분으로 구한 기울기와 오차역전파법으로 구한 기울기의 오차를 확인한다. 여기서는 각 가중치 매개변수의 차이의 절댓값을 구하고, 이를 평균한 값이 오차가 된다.

결과를 보면 기울기의 차이가 매우 적음을 알 수 있고, 오차역전파법이 잘 구현되었다고 생각할 수 있다.

#### **오차역전파법을 사용한 학습 구현**
```python
import numpy as np
from dataset.mnist import load_mnist

(x_train, t_train), (x_test, t_test) = load_mnist(normalize=True, one_hot_label=True)
network = TwoLayerNet(input_size=784, hidden_size=50, output_size=10)

iters_num = 10000
train_size = x_train.shape[0]
batch_size = 100
learning_rate = 0.1

train_loss_list = []
train_acc_list = []
test_acc_list = []

iter_per_epoch = max(train_size / batch_size, 1)

for i in range(iters_num):
    batch_mask = np.random.choice(train_size, batch_size)
    x_batch = x_train[batch_mask]
    t_batch = t_train[batch_mask]
    
    # 오차역전파법으로 기울기 구함
    grad = network.gradient(x_batch, t_batch)
    
    # 갱신
    for key in ('W1', 'b1', 'W2', 'b2'):
        network.params[key] -= learning_rate * grad[key]
        
    loss = network.loss(x_batch, t_batch)
    train_loss_list.append(loss)
    
    if i % iter_per_epoch == 0:
        train_acc = network.accuracy(x_train, t_train)
        test_acc = network.accuracy(x_test, t_test)
        train_acc_list.append(train_acc)
        test_acc_list.append(test_acc)
        print(train_acc, test_acc)    
```
> 0.1532 0.1537\
0.9044166666666666 0.9073\
0.9226666666666666 0.9233\
0.9356666666666666 0.9353\
0.9429 0.9402\
0.9493833333333334 0.9458\
0.95535 0.9526\
0.96005 0.9556\
0.9636833333333333 0.9584\
0.9663 0.9596\
0.9679833333333333 0.9617\
0.9715833333333334 0.9656\
0.974 0.967\
0.97465 0.9672\
0.97565 0.967\
0.9776166666666667 0.9714\
0.9796833333333334 0.9719

실행시켜보면 이전과는 비교도 안될 정도로 빨라졌다.

그래프로 보면 아래와 같다.
```python
import matplotlib.pyplot as plt

markers = {'train': 'o', 'test': 's'}
x = np.arange(len(train_acc_list))
plt.plot(x, train_acc_list, label='train acc')
plt.plot(x, test_acc_list, label='test acc', linestyle='--')
plt.xlabel("epochs")
plt.ylabel("accuracy")
plt.ylim(0, 1.0)
plt.legend(loc='lower right')
plt.show()
```
![train](https://user-images.githubusercontent.com/38313522/153374690-29d7cd35-c151-4990-8ddb-61599927990f.PNG)

----

## 6. 학습 관련 기술들
### 매개변수 갱신
신경망 학습의 목적은 손실 함수의 값을 가능한 한 잔추는 매개변수를 찾는 것으로 이는 매개변수의 최적값을 찾는 문제이며, 이 문제를 푸는 것을 최적화라고 한다.

#### **확률적 경사 하강법(SGD)**
SDG의 수식은 다음과 같다.

<img width="167" alt="e 6 1" src="https://user-images.githubusercontent.com/38313522/153375465-cc17fdc0-68b6-43d5-89b9-d2de5af6720e.png">

여기서 W는 갱신할 가중치 매개변수고, dL/dW는 W에 대한 손실함수의 기울기이다. η는 학습률을 의미하는데, 실제로는 0.01이나 0.001과 같은 값을 미리 정해서 사용한다. ←는 우변의 값으로 좌변의 값을 갱신한다는 뜻이다.

즉, SGD는 기울어진 방향으로 일정 거리만 가겠다는 단순한 방법이다.

```python
class SGD:
    def __init__(self, lr=0.01):
        self.lr = lr
        
    def update(self, params, grads):
        for key in params.keys():
            params[key] -= self.lr * grads[key]
```
lr은 학습률을 뜻하고, 이를 인스턴스 변수로 유지한다. 

#### **SGD의 단점**
SGD는 단순하고 구현도 쉽지만, 문제에 따라서는 비효율적일 때가 있다.

다음 함수의 최솟값을 구하는 문제에서 단점을 알 수 있다.

<img width="191" alt="e 6 2" src="https://user-images.githubusercontent.com/38313522/153376711-5afa6264-62d8-4aa7-866e-36ae22c14382.png">

위의 식을 그림으로 나타내면 다음과 같이 표현된다.

<img width="648" alt="fig 6-1" src="https://user-images.githubusercontent.com/38313522/153376761-aebd37c0-599d-4b1e-be53-172bed6e3156.png">

이 식의 함수의 기울기를 그려보면 다음의 그림처럼 된다.

<img width="551" alt="fig 6-2" src="https://user-images.githubusercontent.com/38313522/153376887-cd1584f5-cc3c-4995-9d87-22633426b0fd.png">

이 기울기는 y축 방향은 크고 x축 방향은 작다는 것이 특징이다. 즉, y축 방향은 가파른데 x축 방향은 완만하다는 것이다. 

이 식이 최솟값이 되는 장소는 (x, y) = (0, 0)이지만, 기울기 대부분이 (0, 0) 방향을 가리키지 않는다.

이 함수에 초깃값으로 (x, y) = (-7.0, 2.0)을 주고 SGD를 적용하면 다음과 같이 표현된다.

<img width="563" alt="fig 6-3" src="https://user-images.githubusercontent.com/38313522/153377660-46dbdfee-4401-4a76-9967-3ec3cb0af84e.png">

최솟값인 (0, 0)까지 지그재그로 이동하는 것을 볼 수 있다.

SGD의 단점은 비등방성(anisotropy) 함수(방향에 따라 성질, 즉 여기에서는 기울기가 달라지는 함수)에서는 탐색 경로가 비효율적이라는 것이다.

#### **모멘텀(Momentum)**
모멘텀 기법은 수식으로 다음과 같이 쓸 수 있다.

<img width="161" alt="e 6 3" src="https://user-images.githubusercontent.com/38313522/153378342-55ed982c-76c5-4f19-aacf-6a03777d170a.png">

<img width="130" alt="e 6 4" src="https://user-images.githubusercontent.com/38313522/153378361-a07932dc-4016-4b00-84e5-bd241aae3a6e.png">

W는 갱신할 가중치 매개변수, dL/dW는 W에 대한 손실 함수의 기울기, η는 학습률, v는 속도이다. 

첫 번째 식은 기울기 방향으로 힘을 받아 물체가 가속된다는 물리 법칙을 나타낸다.

모멘텀은 아래의 그림과 같이 공이 그릇의 바닥을 구르는 것 같은 움직임을 보여준다.

<img width="629" alt="fig 6-4" src="https://user-images.githubusercontent.com/38313522/153378850-028d6df5-c20f-4bfd-a1cb-ce014f974b7d.png">

첫 번째 식의 αv는 물체가 아무런 힘을 받지 않을 때 서서히 하강시키는 역할을 한다.(α는 0.9 등의 값으로 설정한다.) 물리에서 마찰, 공기 저항에 해당한다.

```python
class Momentum:
    def __init__(self, lr=0.01, momentum=0.9):
        self.lr = lr
        self.momentum = momentum
        self.v = None
        
    def update(self, params, grads):
        if self.v is None:
            self.v = {}
            for key, val in params.items():
                self.v[key] = np.zeros_like(val)
                
        for key in params.keys():
            self.v[key] = self.momentum*self.v[key] - self.lr*grads[key]
            params[key] += self.v[key]
```
인스턴스 변수 v는 물체의 속도로 초기화 때는 아무 값도 담지 않고, update()가 처음 호출될 때 매개변수와 같은 구조의 데이터를 딕셔너리 변수로 저장한다.

모멘텀을 SGD를 적용했던 곳에 적용해보면 다음과 같이 최적화 갱신 경로가 그려진다.

<img width="557" alt="fig 6-5" src="https://user-images.githubusercontent.com/38313522/153379966-7697272d-cc55-4f9d-b974-5e68135995c1.png">

SGD보다 지그재그한 정도가 덜하다. 이는 x축의 힘은 아주 작지만 방향은 변하지 않아서 한 방향으로 일정하게 가속하기 때문이다. 반대로 y축의 힘은 크지만 위아래로 번갈아 받아서 상충하여 y축 방향의 속도는 안정적이지 않다.  
그래서 전체적으로 SGD보다 x축 방향으로 빠르게 다가가서 지그재그가 줄어든다.

#### **AdaGrad**
신경망 학습에서는 학습률이 너무 작으면 학습 시간이 너무 길어지고, 반대로 너무 크면 발산하여 학습이 제대로 이뤄지지 않는다.

이 학습률을 정하는 효과적 기술로 학습률 감소(learning rate decay)가 있다. 학습을 진행하면서 학습률을 점차 줄여가는 방법이다.

학습률을 서서히 낮추는 가장 간단한 방법인 매개변수 '전체'의 학습률 값을 일괄적으로 낮추는 것을 발전시킨 것이 AdaGrad이다. 

AdaGrad는 개별 매개변수에 적응적으로 학습률을 조정하면서 학습을 진행한다. 갱신 방법을 수식으로 표현하면 다음과 같다.

<img width="189" alt="e 6 5" src="https://user-images.githubusercontent.com/38313522/153384074-f128fba6-4117-4610-ad9d-508cc54229f1.png">

<img width="198" alt="e 6 6" src="https://user-images.githubusercontent.com/38313522/153384094-9ea75081-bd88-48a8-ae3a-6f671371ee49.png">

h는 기존 기울기 값을 제곱하여 계속 더해준다. ⨀는 행렬의 원소별 곱셈을 의미한다. 

매개변수를 갱신할 때 1/sqrt(h)를 곱해 학습률을 조정한다. 이는 매개변수의 원소 중 크게 갱신된 원소는 학습률이 낮아진다는 것으로, 학습률 감소가 매개변수의 원소마다 다르게 적용됨을 뜻한다.

AdaGrad는 과거의 기울기를 제곱하여 계속 더해가서 학습을 진행할수록 갱신 강도가 약해져 어느 순간 갱신량이 0이 되어 전혀 갱신하지 않게 된다. 이를 개선한 것이 RMSProp으로 모든 기울기를 균일하게 더해가는 것이 아니라, 먼 과거의 기울기는 서서히 잊고 새로운 기울기 정보를 크게 반영한다. 이를 지수이동평균(Exponential Moving Average, EMA)이라 하여, 과거 기울기의 반영 규모를 기하급수적으로 감소시킨다.

```python
class AdaGrad:
    def __init__(self, lr=0.01):
        self.lr = lr
        self.h = None
        
    def update(self, params, grads):
        if self.h is None:
            self.h = {}
            for key, val in params.items():
                self.h[key] = np.zeros_like(val)
                
        for key in params.keys():
            self.h[key] += grads[key] * grads[key]
            params[key] -= self.lr * grads[key] / (np.sqrt(self.h[key]) + 1e-7)
```
1e-7이라는 작은 값을 더해서 self.h[key]에 0이 담겨 있다 해도 0으로 나누는 사태를 막아주는 것을 주의해야 한다.

AdaGrad를 적용해서 최적화 문제를 풀면 다음과 같은 결과가 나온다.

<img width="558" alt="fig 6-6" src="https://user-images.githubusercontent.com/38313522/153385671-255960a0-d1ea-482a-a1e5-6805ead5d9bb.png">

y축 방향은 기울기가 커서 처음에는 크게 움직이지만, 그 큰 움직임에 비례해 갱신 정도도 큰 폭으로 작아지도록 조정된다. 그래서 y축 방향으로 갱신 강도가 빠르게 약해지고, 지그재그 움직임이 줄어든다.

#### **Adam**
Adam은 모멘텀과 AdaGrad를 융합한 것 같은 방법이다.

하이퍼파라미터의 '편향 보정'이 진행된다는 특징이 있다.

아래는 책의 github에 올라와 있는 Adam 클래스이다.
```python
class Adam:

    """Adam (http://arxiv.org/abs/1412.6980v8)"""

    def __init__(self, lr=0.001, beta1=0.9, beta2=0.999):
        self.lr = lr
        self.beta1 = beta1
        self.beta2 = beta2
        self.iter = 0
        self.m = None
        self.v = None
        
    def update(self, params, grads):
        if self.m is None:
            self.m, self.v = {}, {}
            for key, val in params.items():
                self.m[key] = np.zeros_like(val)
                self.v[key] = np.zeros_like(val)
        
        self.iter += 1
        lr_t  = self.lr * np.sqrt(1.0 - self.beta2**self.iter) / (1.0 - self.beta1**self.iter)         
        
        for key in params.keys():
            #self.m[key] = self.beta1*self.m[key] + (1-self.beta1)*grads[key]
            #self.v[key] = self.beta2*self.v[key] + (1-self.beta2)*(grads[key]**2)
            self.m[key] += (1 - self.beta1) * (grads[key] - self.m[key])
            self.v[key] += (1 - self.beta2) * (grads[key]**2 - self.v[key])
            
            params[key] -= lr_t * self.m[key] / (np.sqrt(self.v[key]) + 1e-7)
            
            #unbias_m += (1 - self.beta1) * (grads[key] - self.m[key]) # correct bias
            #unbisa_b += (1 - self.beta2) * (grads[key]*grads[key] - self.v[key]) # correct bias
            #params[key] += self.lr * unbias_m / (np.sqrt(unbisa_b) + 1e-7)
```
Adam으로 최적화 문제를 풀면 다음과 같은 결과가 나온다.

<img width="560" alt="fig 6-7" src="https://user-images.githubusercontent.com/38313522/153388379-b18ca7c1-eb6c-4326-b615-3fa1362d6094.png">

Adam은 하이퍼파라미터를 3개 설정한다. 하나는 학습률(논문에서는 α), 나머지 두 개는 일차 모멘텀용 계수 β1과 이차 모멘텀용 계수 β2이다. 논문에 따르면 기본 설정값은 β1은 0.9, β2는 0.999이고, 이 값이면 많은 경우에 좋은 결과를 얻을 수 있다.

자세한 것은 [논문](https://arxiv.org/pdf/1412.6980.pdf)을 참조하면 된다.

#### **MNIST 데이터셋에서 비교**
```python
def smooth_curve(x):
    """손실 함수의 그래프를 매끄럽게 하기 위해 사용
    
    참고：http://glowingpython.blogspot.jp/2012/02/convolution-with-numpy.html
    """
    window_len = 11
    s = np.r_[x[window_len-1:0:-1], x, x[-1:-window_len:-1]]
    w = np.kaiser(window_len, 2)
    y = np.convolve(w/w.sum(), s, mode='valid')
    return y[5:len(y)-5]
```
위는 마지막에 그래프를 매끄럽게 하기 위해 사용하는 함수라고 한다.

```python
class MultiLayerNet:
    """완전연결 다층 신경망
    Parameters
    ----------
    input_size : 입력 크기（MNIST의 경우엔 784）
    hidden_size_list : 각 은닉층의 뉴런 수를 담은 리스트（e.g. [100, 100, 100]）
    output_size : 출력 크기（MNIST의 경우엔 10）
    activation : 활성화 함수 - 'relu' 혹은 'sigmoid'
    weight_init_std : 가중치의 표준편차 지정（e.g. 0.01）
        'relu'나 'he'로 지정하면 'He 초깃값'으로 설정
        'sigmoid'나 'xavier'로 지정하면 'Xavier 초깃값'으로 설정
    weight_decay_lambda : 가중치 감소(L2 법칙)의 세기
    """
    def __init__(self, input_size, hidden_size_list, output_size,
                 activation='relu', weight_init_std='relu', weight_decay_lambda=0):
        self.input_size = input_size
        self.output_size = output_size
        self.hidden_size_list = hidden_size_list
        self.hidden_layer_num = len(hidden_size_list)
        self.weight_decay_lambda = weight_decay_lambda
        self.params = {}

        # 가중치 초기화
        self.__init_weight(weight_init_std)

        # 계층 생성
        activation_layer = {'sigmoid': Sigmoid, 'relu': Relu}
        self.layers = OrderedDict()
        for idx in range(1, self.hidden_layer_num+1):
            self.layers['Affine' + str(idx)] = Affine(self.params['W' + str(idx)],
                                                      self.params['b' + str(idx)])
            self.layers['Activation_function' + str(idx)] = activation_layer[activation]()

        idx = self.hidden_layer_num + 1
        self.layers['Affine' + str(idx)] = Affine(self.params['W' + str(idx)],
            self.params['b' + str(idx)])

        self.last_layer = SoftmaxWithLoss()

    def __init_weight(self, weight_init_std):
        """가중치 초기화
        
        Parameters
        ----------
        weight_init_std : 가중치의 표준편차 지정（e.g. 0.01）
            'relu'나 'he'로 지정하면 'He 초깃값'으로 설정
            'sigmoid'나 'xavier'로 지정하면 'Xavier 초깃값'으로 설정
        """
        all_size_list = [self.input_size] + self.hidden_size_list + [self.output_size]
        for idx in range(1, len(all_size_list)):
            scale = weight_init_std
            if str(weight_init_std).lower() in ('relu', 'he'):
                scale = np.sqrt(2.0 / all_size_list[idx - 1])  # ReLU를 사용할 때의 권장 초깃값
            elif str(weight_init_std).lower() in ('sigmoid', 'xavier'):
                scale = np.sqrt(1.0 / all_size_list[idx - 1])  # sigmoid를 사용할 때의 권장 초깃값
            self.params['W' + str(idx)] = scale * np.random.randn(all_size_list[idx-1], all_size_list[idx])
            self.params['b' + str(idx)] = np.zeros(all_size_list[idx])

    def predict(self, x):
        for layer in self.layers.values():
            x = layer.forward(x)

        return x

    def loss(self, x, t):
        """손실 함수를 구한다.
        
        Parameters
        ----------
        x : 입력 데이터
        t : 정답 레이블 
        
        Returns
        -------
        손실 함수의 값
        """
        y = self.predict(x)

        weight_decay = 0
        for idx in range(1, self.hidden_layer_num + 2):
            W = self.params['W' + str(idx)]
            weight_decay += 0.5 * self.weight_decay_lambda * np.sum(W ** 2)

        return self.last_layer.forward(y, t) + weight_decay

    def accuracy(self, x, t):
        y = self.predict(x)
        y = np.argmax(y, axis=1)
        if t.ndim != 1 : t = np.argmax(t, axis=1)

        accuracy = np.sum(y == t) / float(x.shape[0])
        return accuracy

    def numerical_gradient(self, x, t):
        """기울기를 구한다(수치 미분).
        
        Parameters
        ----------
        x : 입력 데이터
        t : 정답 레이블
        
        Returns
        -------
        각 층의 기울기를 담은 딕셔너리(dictionary) 변수
            grads['W1']、grads['W2']、... 각 층의 가중치
            grads['b1']、grads['b2']、... 각 층의 편향
        """
        loss_W = lambda W: self.loss(x, t)

        grads = {}
        for idx in range(1, self.hidden_layer_num+2):
            grads['W' + str(idx)] = numerical_gradient(loss_W, self.params['W' + str(idx)])
            grads['b' + str(idx)] = numerical_gradient(loss_W, self.params['b' + str(idx)])

        return grads

    def gradient(self, x, t):
        """기울기를 구한다(오차역전파법).
        Parameters
        ----------
        x : 입력 데이터
        t : 정답 레이블
        
        Returns
        -------
        각 층의 기울기를 담은 딕셔너리(dictionary) 변수
            grads['W1']、grads['W2']、... 각 층의 가중치
            grads['b1']、grads['b2']、... 각 층의 편향
        """
        # forward
        self.loss(x, t)

        # backward
        dout = 1
        dout = self.last_layer.backward(dout)

        layers = list(self.layers.values())
        layers.reverse()
        for layer in layers:
            dout = layer.backward(dout)

        # 결과 저장
        grads = {}
        for idx in range(1, self.hidden_layer_num+2):
            grads['W' + str(idx)] = self.layers['Affine' + str(idx)].dW + self.weight_decay_lambda * self.layers['Affine' + str(idx)].W
            grads['b' + str(idx)] = self.layers['Affine' + str(idx)].db

        return grads
```
이것은 책에서 제공한 다층 신경망 클래스이다. 이번 실험에 사용되어 가져왔다.

```python
# 0. MNIST 데이터 읽기==========
(x_train, t_train), (x_test, t_test) = load_mnist(normalize=True, one_hot_label=True)

train_size = x_train.shape[0]
batch_size = 128
max_iterations = 2000


# 1. 실험용 설정==========
optimizers = {}
optimizers['SGD'] = SGD()
optimizers['Momentum'] = Momentum()
optimizers['AdaGrad'] = AdaGrad()
optimizers['Adam'] = Adam()

networks = {}
train_loss = {}
for key in optimizers.keys():
    networks[key] = MultiLayerNet(
        input_size=784, hidden_size_list=[100, 100, 100, 100],
        output_size=10)
    train_loss[key] = []    


# 2. 훈련 시작==========
for i in range(max_iterations):
    batch_mask = np.random.choice(train_size, batch_size)
    x_batch = x_train[batch_mask]
    t_batch = t_train[batch_mask]
    
    for key in optimizers.keys():
        grads = networks[key].gradient(x_batch, t_batch)
        optimizers[key].update(networks[key].params, grads)
    
        loss = networks[key].loss(x_batch, t_batch)
        train_loss[key].append(loss)
    
    if i % 100 == 0:
        print( "===========" + "iteration:" + str(i) + "===========")
        for key in optimizers.keys():
            loss = networks[key].loss(x_batch, t_batch)
            print(key + ":" + str(loss))


# 3. 그래프 그리기==========
markers = {"SGD": "o", "Momentum": "x", "AdaGrad": "s", "Adam": "D"}
x = np.arange(max_iterations)
for key in optimizers.keys():
    plt.plot(x, smooth_curve(train_loss[key]), marker=markers[key], markevery=100, label=key)
plt.xlabel("iterations")
plt.ylabel("loss")
plt.ylim(0, 1)
plt.legend()
plt.show()
```
위가 MNIST 데이터셋으로 실험을 진행한 것이다. 코드는 책에서 공개한 github에 있는 것이다.

![compare_mnist](https://user-images.githubusercontent.com/38313522/153394729-b17265dd-de0b-4452-8b00-5038398ce25b.PNG)


각 층이 100개의 뉴런으로 구성된 5층 신경망에서 ReLU를 활성화 함수로 사용했다.

결과를 보면 각각의 학습 진도를 알 수 있다. SGD가 가장 느리고 나머지는 비슷하다.