# Day 6
## 직장인을 위한 데이터분석 with 파이썬 (6일차)
### Starbucks 매장 목록 크롤링

스타벅스 : https://www.starbucks.co.kr/

**스타벅스 매장 정보 크롤링**

```python
from selenium import webdriver
from bs4 import BeautifulSoup
import pandas as pd

browser = webdriver.Chrome('c:/webdriver/chromedriver.exe')
url = 'https://www.starbucks.co.kr/store/store_map.do?disp=locale' # disp=locale은 지역 검색이 선택된 상태의 url
browser.get(url)

# 서울 클릭
seoul_btn = '#container > div > form > fieldset > div > section > article.find_store_cont > article > article:nth-child(4) > div.loca_step1 > div.loca_step1_cont > ul > li:nth-child(1) > a'
browser.find_element_by_css_selector(seoul_btn).click()

# '전체' 클릭
all_btn = '#mCSB_2_container > ul > li:nth-child(1) > a'
browser.find_element_by_css_selector(all_btn).click()

# HTML 정보 받아와서 파싱
html = browser.page_source
soup = BeautifulSoup(html, 'html.parser')

# 서울 스타먹스 모든 매장 목록
starbucks_soup_list = soup.select('li.quickResultLstCon')

# 각 매장의 정보 추출
starbucks_list = []
for item in starbucks_soup_list:
    name = item.select('strong')[0].text.strip() # 매장 이름
    lat = item['data-lat'].strip() # 위도
    lng = item['data-long'].strip() # 경도
    store_type = item.select('i')[0]['class'][0][4:] # 매장 타입(일반 매장, 리저브 매장, 드라이브 스루)
    address = str(item.select('p.result_details')[0]).split('<br/>')[0].split('>')[1] # 주소
    tel = str(item.select('p.result_details')[0]).split('<br/>')[1].split('<')[0] # 전화번호
    
    starbucks_list.append([name, lat, lng, store_type, address, tel])

    columns = ['매장명', '위도', '경도', '매장타입','주소', '전화번호']
seoul_starbucks_df = pd.DataFrame(starbucks_list, columns = columns)

seoul_starbucks_df.to_excel('./files/1_seoul_starbucks_list.xlsx', index = False)
```
이번에는 크롬 개발자 도구에서 우클릭 > copy selector를 활용하여 코드를 작성했다.


---
### 스타벅스 매장 정보와 서울시 데이터 전처리
이 전에 원래 서울시 OPEN API를 사용하여 시군구 데이터를 받아와 처리하는 부분이 책에 있었지만 해당 API를 찾을 수 없어서 예제파일을 활용

**데이터 전처리**
```python
import pandas as pd

# 서울 스타벅스 매장 정보 가져오기
seoul_starbucks = pd.read_excel('./files/1_seoul_starbucks_list.xlsx', header = 0)

# 매장 주소 정보에서 시군구명 추출, '시군구명' 칼럼 추가
sgg_names = []
for address in seoul_starbucks['주소']:
    sgg = address.split()[1]
    sgg_names.append(sgg)
seoul_starbucks['시군구명'] = sgg_names

# 서울 시군구 목록 데이터 가져오기
seoul_sgg = pd.read_excel('./files/seoul_sgg_list.xlsx')

# 시군구별 스타벅스 매장 수 세기
starbucks_sgg_count = seoul_starbucks.pivot_table(
    index = '시군구명', # 그룹화할 칼럼명
    values = '매장명', # 결과에 포함할 칼럼명
    aggfunc = 'count').rename(columns = {'매장명':'스타벅스_매장수'}) # 결과 데이터의 칼럼명 변경

# 서울 시군구 목록 데이터에 스타벅스 매장 수 데이터를 병합
seoul_sgg = pd.merge(seoul_sgg,
                     starbucks_sgg_count,
                     how = 'left',        # seoul_sgg 기준으로 병합
                     on = '시군구명')     # left = '시군구명', right = '시군구명'과 같은 역할, 서로 매칭되는 칼럼명으로 '시군구명' 지정

# 서울시 시군구별 인구통계 데이터 불러오기 및 시군구 목록 데이터에 병합
seoul_sgg_pop = pd.read_excel('./files/sgg_pop.xlsx')

seoul_sgg = pd.merge(seoul_sgg,
                     seoul_sgg_pop,
                     how = 'left',
                     on = '시군구명')

seoul_sgg_biz = pd.read_excel('./files/sgg_biz.xlsx')
seoul_sgg = pd.merge(seoul_sgg,
                    seoul_sgg_biz,
                    how = 'left',
                    on = '시군구명')                   

seoul_sgg.to_excel('./files/1_seoul_sgg_stat.xlsx', index = False)                      
```