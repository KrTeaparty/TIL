# Day 5
## 직장인을 위한 데이터분석 with 파이썬 (5일차)
### 해시태그로 워드 클라우드 제작
**해시태그 데이터 만들기**
```python
import pandas as pd
from collections import Counter

raw_total = pd.read_excel('./files/1_crawling_raw.xlsx')

tags_total = []

for tags in raw_total['tags']:
    tags_list = tags[2:-2].split("', '") # 시작과 끝에 있는 [] 기호 제거, 콤마 기준으로 해시태그 나눔
    for tag in tags_list:
        tags_total.append(tag)

# 해시태그 출현 빈도 집계
tag_counts = Counter(tags_total)

# 데이터 정제 (무관한 해시태그 제외)
STOPWORDS = ['#일상',
            '#선팔',
            '#제주도',
            '#jeju',
            '#반영구',
            '#제주자연눈썹',
            '#서귀포눈썹문신',
            '#제주눈썹문신',
            '#소통',
            '#맞팔',
            '',
            '#눈썹문신',
            '#제주속눈썹',
            '#서귀포남자눈썹문신',
            '#서귀포속눈썹',
            '#서귀포자연눈썹',
            '#서귀포반영구',
            '#제주메이크업',
            '#제주남자눈썹문신']
tag_total_selected = []
for tag in tags_total:
    if tag not in STOPWORDS:
        tag_total_selected.append(tag)
        
tag_counts_selected = Counter(tag_total_selected)
```

**Barplot으로 해시태그 보기**
```python
import matplotlib.pyplot as plt
import seaborn as sns
from matplotlib import font_manager, rc
import sys

if sys.platform in ['win32','win64']:
    font_name = 'malgun gothic'
    
rc('font',family=font_name)

tag_counts_df = pd.DataFrame(tag_counts_selected.most_common(30))
tag_counts_df.columns = ['tags', 'counts']

plt.figure(figsize = (10, 8))
sns.barplot(x = 'counts', y = 'tags', data = tag_counts_df)
```
**워드 클라우드 제작**
```python
import matplotlib.pyplot as plt
from wordcloud import WordCloud
import platform

# 워드 클라우드에서는 글꼴의 경로를 직접 지정해야 함
if platform.system() == 'Windows':
    font_path = 'c:/Windows/Fonts/malgun.ttf'

wordcloud = WordCloud(font_path = font_path,       # 사용하려는 글꼴의 경로
                     background_color = 'white',   # 배경색
                     max_words = 100,              # 최대 표현할 단어 수
                     relative_scaling = 0.3,       # 워드 클라우드 내 글자들의 상대적인 크기 (0~1까지 지정가능, 0에 가까울수록 빈도수의 순위에, 1에 가까울수록 빈도수에 더 큰 영향을 보임)
                     width = 800,                  # 가로 크기
                     height = 400                  # 세로 크기
                     ).generate_from_frequencies(tag_counts_selected)
plt.figure(figsize = (15, 10))
plt.imshow(wordcloud)
plt.axis('off')
plt.savefig('./files/3_2_tag-wordcloud.png')
```
Wordcloud는 이전에 한번 만든 적이 있었는데 그 때 여러 옵션이 추가 가능했는데 나중에 따로 Wordcloud도 한 번 다룰 필요가 있다고 판단

---
### 지도 시각화
**데이터 준비**
```python
import pandas as pd
raw_total = pd.read_excel('./files/1_crawling_raw.xlsx')

location_counts = raw_total['place'].value_counts()

location_counts_df = pd.DataFrame(location_counts)

location_counts_df.to_excel('./files/3_3_location_counts.xlsx')

locations = list(location_counts.index)
```

**카카오 로컬 API 활용 장소 검색 함수화**
```python
def find_places(searching):
    # 접속 URL 만들기
    url = 'https://dapi.kakao.com/v2/local/search/keyword.json?query={}'.format(searching)
    
    # headers 입력하기
    headers = {
        'Authorization' : 'KakaoAK <API 키>'
    }
    
    # API 요청 및 정보 받기
    places = requests.get(url, headers = headers).json()['documents']
    
    # 필요한 정보 선택하기
    place = places[0]
    name = place['place_name']
    x = place['x']
    y = place['y']
    data = [name, x, y, searching]
    
    return data
```
**위치 정보 검색 및 저장**
```python
import time
from tqdm import tqdm

locations_inform = []
for location in tqdm(locations): # tqdm은 프로그레스바를 보여줌
    try: # 검색되지 않는 곳일 경우 에러가 발생하므로 try except문 사용
        data = find_places(location)
        locations_inform.append(data)
        time.sleep(0.5) # 과도하게 빠른 요청 때문에  API 서버에서 접근을 차단하는 것을 방지
    except:
        pass

locations_inform_df = pd.DataFrame(locations_inform)
locations_inform_df.columns = ['네이버위치명', '경도', '위도', '인스타위치명']
locations_inform_df.to_excel('./files/3_3_locations.xlsx', index = False)
```
tqdm은 프로그레스바로 진행상황을 가시화해준다.

tqdm(iterable)로 사용

trange == tqdm(range(~))

**위치 정보별 인스타 게시량 정리**
```python
location_counts_df = pd.read_excel('./files/3_3_location_counts.xlsx', index_col = 0) # 첫 번째 칼럼을 인덱스로 사용
locations_inform_df = pd.read_excel('./files/3_3_locations.xlsx')

location_data = pd.merge(locations_inform_df,      # location_inform_df와 location_counts_df를 병합
                        location_counts_df,
                        how = 'inner',             # 양쪽에 모두 있는 데이터만 포함
                        left_on = '네이버위치명',  # locations_inform_df 데이터에서 네이버위치명 칼럼의 데이터를 기준으로 병합
                        right_index = True)       # location_counts_df 데이터에서 인덱스를 기준으로 병합

# 중복 데이터 처리
location_data = location_data.pivot_table(
    index = ['네이버위치명', '경도', '위도'], # 네이버위치명, 경도, 위도가 모두 동일할 경우 place 값을 병합
    values = 'place',
    aggfunc='sum')

location_data.to_excel('./files/3_location_inform.xlsx', index = True)
```
**folium을 이용한 지도 시각화**
- 개별 표시
```python
import folium

location_data = pd.read_excel('./files/3_location_inform.xlsx')

Mt_Hanla = [33.362500, 126.533694]
map_jeju = folium.Map(location = Mt_Hanla, zoom_start = 11) # 지도 생성, location = Mt_Hanla 옵션으로 한라산 좌표를 중심으로 지도 표시

for i in range(len(location_data)):
    name = location_data['네이버위치명'][i]   # 공식 명칭
    count = location_data['place'][i]         # 게시글 개수
    size = int(count)*2                       # 게시 횟수의 두배
    long = float(location_data['위도'][i])
    lat = float(location_data['경도'][i])
    folium.CircleMarker((long, lat), radius = size, colort='red', popup=name).add_to(map_jeju)
    # 원을 생성, 원의 위치(경도/위도), 원 크기, 색상, 원을 선택했을 때 이름 지정
    
map_jeju.save('./files/3_jeju.html')
```
- 그룹 표시
```python
from folium.plugins import MarkerCluster

locations = []
names = []

for i in range(len(location_data)):
    data = location_data.iloc[i] # 행 하나씩
    locations.append((float(data['위도']), float(data['경도']))) # 위도, 경도 순으로
    names.append(data['네이버위치명'])
    
Mt_Hanla = [33.362500, 126.533694]
map_jeju2 = folium.Map(location = Mt_Hanla, zoom_start = 11)

marker_cluster = MarkerCluster(
    locations = locations, popups = names,
    name = 'Jeju',
    overlay = True,
    control = True
)

marker_cluster.add_to(map_jeju2)
folium.LayerControl().add_to(map_jeju2)

map_jeju2.save('./files/3_jeju_cluster.html')
```
 API를 처음 사용해봤는데 나중에 다른 API도 사용해서 익숙해질 필요가 있다고 판단

---
### 특정 단어를 포함한 게시글 찾기
```python
import pandas as pd
raw_total = pd.read_excel('./files/1_crawling_raw.xlsx')

select_word_list = ['해돋이', '박물관', '힐링', '게스트하우스', '섭지코지']

for select_word in select_word_list:
    check_list = []
    for content in raw_total['content']:
        if select_word in content:
            check_list.append(True)
        else:
            check_list.append(False)
            
    select_df = raw_total[check_list]
    fpath = f'./files/4_select_data_{select_word}.xlsx'
    select_df.to_excel(fpath)
```
1_crwaling_raw 파일에서 nan이 하나 발견되었는데 저번에 크롤링할 때 본문을 가져오는 부분에서 내가 본문에 내용이 없으면 ' '이 아니라 ''로 처리를 해서 for문을 돌릴 때 에러가 발생했었다.